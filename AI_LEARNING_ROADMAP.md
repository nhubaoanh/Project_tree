# üöÄ L·ªò TR√åNH H·ªåC AI - 3 TH√ÅNG (Machine Learning ‚Üí Deep Learning ‚Üí NLP ‚Üí CV)

> **L·ªô tr√¨nh h·ªçc AI t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao, d·ª±a tr√™n d·ª± √°n Family Tree th·ª±c t·∫ø**
> 
> **Th·ªùi gian**: 3 th√°ng (12 tu·∫ßn)
> **M·ª•c ti√™u**: Hi·ªÉu v√† √°p d·ª•ng AI v√†o d·ª± √°n th·ª±c t·∫ø
> **Ph∆∞∆°ng ph√°p**: L√Ω thuy·∫øt + Th·ª±c h√†nh + B√†i t·∫≠p

---

## üìã M·ª§C L·ª§C

- [T·ªïng quan l·ªô tr√¨nh 3 th√°ng](#t·ªïng-quan-l·ªô-tr√¨nh-3-th√°ng)
- [TH√ÅNG 1: Machine Learning & Deep Learning C∆° b·∫£n](#th√°ng-1-machine-learning--deep-learning-c∆°-b·∫£n)
- [TH√ÅNG 2: Natural Language Processing (NLP)](#th√°ng-2-natural-language-processing-nlp)
- [TH√ÅNG 3: Computer Vision & Advanced Topics](#th√°ng-3-computer-vision--advanced-topics)
- [Ki·∫øn tr√∫c AI trong d·ª± √°n Family Tree](#ki·∫øn-tr√∫c-ai-trong-d·ª±-√°n-family-tree)
- [T√†i nguy√™n h·ªçc t·∫≠p](#t√†i-nguy√™n-h·ªçc-t·∫≠p)

---

## üìÖ T·ªîNG QUAN L·ªò TR√åNH 3 TH√ÅNG

### Th√°ng 1: Machine Learning & Deep Learning C∆° b·∫£n
- **Tu·∫ßn 1-2**: Python, NumPy, Pandas c∆° b·∫£n
- **Tu·∫ßn 3-4**: Machine Learning v·ªõi Scikit-learn
- **Tu·∫ßn 5-6**: Deep Learning v·ªõi PyTorch

### Th√°ng 2: Natural Language Processing
- **Tu·∫ßn 7-8**: NLP c∆° b·∫£n, Tokenization, Embeddings
- **Tu·∫ßn 9-10**: Transformers, Hugging Face
- **Tu·∫ßn 11-12**: Fine-tuning, Text-to-SQL (d·ª± √°n)

### Th√°ng 3: Computer Vision & Advanced
- **Tu·∫ßn 13-14**: Computer Vision c∆° b·∫£n, CNN
- **Tu·∫ßn 15-16**: Object Detection, Image Classification
- **Tu·∫ßn 17-18**: Deployment, Production, T·ªëi ∆∞u



---

# TH√ÅNG 1: MACHINE LEARNING & DEEP LEARNING C∆† B·∫¢N

## üóìÔ∏è TU·∫¶N 1-2: Python & Data Science Foundations

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Python C∆° b·∫£n (n·∫øu ch∆∞a v·ªØng)
- Variables, Data types, Functions
- Lists, Dictionaries, Tuples
- Loops, Conditionals
- Classes, OOP basics

#### 2. NumPy - T√≠nh to√°n s·ªë h·ªçc

```python
import numpy as np

# T·∫°o array
arr = np.array([1, 2, 3, 4, 5])
matrix = np.array([[1, 2], [3, 4]])

# Operations
print(arr * 2)           # [2, 4, 6, 8, 10]
print(arr.mean())        # 3.0
print(matrix.shape)      # (2, 2)

# Indexing & Slicing
print(arr[0:3])          # [1, 2, 3]
print(matrix[0, 1])      # 2

# Broadcasting
arr + 10                 # [11, 12, 13, 14, 15]

# Linear Algebra
a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6], [7, 8]])
print(np.dot(a, b))      # Matrix multiplication
```

#### 3. Pandas - X·ª≠ l√Ω d·ªØ li·ªáu b·∫£ng

```python
import pandas as pd

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('data.csv')
df = pd.read_excel('data.xlsx')

# Xem d·ªØ li·ªáu
print(df.head())         # 5 d√≤ng ƒë·∫ßu
print(df.info())         # Th√¥ng tin columns
print(df.describe())     # Th·ªëng k√™

# Filtering
df[df['age'] > 30]
df[df['name'] == 'John']

# Grouping
df.groupby('city')['salary'].mean()

# Missing values
df.isnull().sum()
df.fillna(0)
df.dropna()

# V√≠ d·ª• v·ªõi Family Tree data
members_df = pd.read_sql("SELECT * FROM thanhvien", connection)
print(members_df['ngheNghiep'].value_counts())
print(members_df.groupby('doiThuoc').size())
```



#### 4. Matplotlib & Seaborn - Visualization

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Line plot
plt.plot([1, 2, 3, 4], [1, 4, 9, 16])
plt.xlabel('X axis')
plt.ylabel('Y axis')
plt.title('My Plot')
plt.show()

# Bar chart
df['ngheNghiep'].value_counts().plot(kind='bar')
plt.title('Ngh·ªÅ nghi·ªáp trong gia ph·∫£')
plt.show()

# Histogram
plt.hist(df['age'], bins=20)
plt.show()

# Heatmap
sns.heatmap(df.corr(), annot=True)
plt.show()
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 1-2

**B√†i 1: Ph√¢n t√≠ch d·ªØ li·ªáu Family Tree**
```python
# K·∫øt n·ªëi database v√† ph√¢n t√≠ch
import pandas as pd
import mysql.connector

# 1. Load d·ªØ li·ªáu thanhvien
# 2. T√≠nh tu·ªïi trung b√¨nh theo ƒë·ªùi
# 3. V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë ngh·ªÅ nghi·ªáp
# 4. T√¨m ng∆∞·ªùi l·ªõn tu·ªïi nh·∫•t m·ªói ƒë·ªùi
# 5. Ph√¢n t√≠ch t·ª∑ l·ªá nam/n·ªØ
```

**B√†i 2: Data Cleaning**
```python
# 1. T√¨m v√† x·ª≠ l√Ω missing values trong thanhvien
# 2. Chu·∫©n h√≥a s·ªë ƒëi·ªán tho·∫°i (format 10 s·ªë)
# 3. T√°ch nƒÉm sinh t·ª´ ngaySinh
# 4. T·∫°o column 'age' t·ª´ ngaySinh
# 5. Export ra CSV
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **NumPy**
   - Official Tutorial: https://numpy.org/doc/stable/user/quickstart.html
   - Video: "NumPy Tutorial" by freeCodeCamp (1h)

2. **Pandas**
   - Official Tutorial: https://pandas.pydata.org/docs/getting_started/intro_tutorials/
   - Video: "Pandas Tutorial" by Corey Schafer (playlist)

3. **Matplotlib**
   - Official Tutorial: https://matplotlib.org/stable/tutorials/index.html

---


## üóìÔ∏è TU·∫¶N 3-4: Machine Learning v·ªõi Scikit-learn

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Machine Learning C∆° b·∫£n

**3 lo·∫°i ML:**
1. **Supervised Learning** (H·ªçc c√≥ gi√°m s√°t)
   - Classification: Ph√¢n lo·∫°i (spam/not spam)
   - Regression: D·ª± ƒëo√°n s·ªë (gi√° nh√†)

2. **Unsupervised Learning** (H·ªçc kh√¥ng gi√°m s√°t)
   - Clustering: Ph√¢n nh√≥m
   - Dimensionality Reduction: Gi·∫£m chi·ªÅu

3. **Reinforcement Learning** (H·ªçc tƒÉng c∆∞·ªùng)
   - Agent h·ªçc qua reward/penalty

#### 2. Quy tr√¨nh ML chu·∫©n

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# 1. Load data
X = df[['feature1', 'feature2', 'feature3']]
y = df['target']

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. Preprocessing
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Train model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# 5. Predict
y_pred = model.predict(X_test_scaled)

# 6. Evaluate
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))
```

#### 3. C√°c thu·∫≠t to√°n c∆° b·∫£n

**A. Linear Regression (H·ªìi quy tuy·∫øn t√≠nh)**

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# D·ª± ƒëo√°n tu·ªïi d·ª±a v√†o ƒë·ªùi
X = df[['doiThuoc']]
y = df['age']

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"MSE: {mean_squared_error(y_test, y_pred)}")
print(f"R2 Score: {r2_score(y_test, y_pred)}")
```

**B. Logistic Regression (Ph√¢n lo·∫°i)**

```python
from sklearn.linear_model import LogisticRegression

# D·ª± ƒëo√°n gi·ªõi t√≠nh d·ª±a v√†o t√™n
X = df[['name_features']]  # C·∫ßn feature engineering
y = df['gioiTinh']

model = LogisticRegression()
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)
```

**C. Decision Tree (C√¢y quy·∫øt ƒë·ªãnh)**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

model = DecisionTreeClassifier(max_depth=3)
model.fit(X_train, y_train)

# Visualize tree
plt.figure(figsize=(20,10))
tree.plot_tree(model, filled=True, feature_names=X.columns)
plt.show()
```

**D. Random Forest (R·ª´ng ng·∫´u nhi√™n)**

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature importance
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
print(importances)
```

**E. K-Means Clustering (Ph√¢n nh√≥m)**

```python
from sklearn.cluster import KMeans

# Ph√¢n nh√≥m th√†nh vi√™n theo tu·ªïi v√† ngh·ªÅ nghi·ªáp
X = df[['age', 'ngheNghiep_encoded']]

kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X)

# Visualize
plt.scatter(df['age'], df['ngheNghiep_encoded'], c=df['cluster'])
plt.xlabel('Age')
plt.ylabel('Occupation')
plt.show()
```



#### 4. Evaluation Metrics (ƒê√°nh gi√° model)

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)

# Classification metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.show()

# Regression metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 3-4

**B√†i 1: D·ª± ƒëo√°n ngh·ªÅ nghi·ªáp**
```python
# D·ª± ƒëo√°n ngh·ªÅ nghi·ªáp d·ª±a v√†o:
# - Tr√¨nh ƒë·ªô h·ªçc v·∫•n
# - ƒê·ªùi th·ª©
# - N∆°i sinh
# 
# Y√™u c·∫ßu:
# 1. Feature engineering (encode categorical)
# 2. Train 3 models: Logistic Regression, Decision Tree, Random Forest
# 3. So s√°nh accuracy
# 4. V·∫Ω confusion matrix
# 5. Feature importance
```

**B√†i 2: Ph√¢n nh√≥m th√†nh vi√™n**
```python
# S·ª≠ d·ª•ng K-Means ƒë·ªÉ ph√¢n nh√≥m th√†nh vi√™n theo:
# - Tu·ªïi
# - S·ªë con
# - ƒê·ªùi th·ª©
#
# Y√™u c·∫ßu:
# 1. Chu·∫©n h√≥a d·ªØ li·ªáu
# 2. T√¨m s·ªë cluster t·ªëi ∆∞u (Elbow method)
# 3. Visualize clusters
# 4. Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm m·ªói cluster
```

**B√†i 3: D·ª± ƒëo√°n tu·ªïi th·ªç**
```python
# D·ª± ƒëo√°n tu·ªïi th·ªç (ngayMat - ngaySinh) d·ª±a v√†o:
# - Ngh·ªÅ nghi·ªáp
# - N∆°i sinh
# - Tr√¨nh ƒë·ªô h·ªçc v·∫•n
#
# Y√™u c·∫ßu:
# 1. Linear Regression
# 2. Random Forest Regression
# 3. So s√°nh MSE, RMSE, R2
# 4. Plot predicted vs actual
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **Scikit-learn**
   - Official Tutorial: https://scikit-learn.org/stable/tutorial/index.html
   - User Guide: https://scikit-learn.org/stable/user_guide.html

2. **Courses**
   - Andrew Ng - Machine Learning (Coursera) - HIGHLY RECOMMENDED
   - StatQuest with Josh Starmer (YouTube) - Gi·∫£i th√≠ch tr·ª±c quan

3. **Books**
   - "Hands-On Machine Learning" by Aur√©lien G√©ron
   - "Python Machine Learning" by Sebastian Raschka

---


## üóìÔ∏è TU·∫¶N 5-6: Deep Learning v·ªõi PyTorch

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Neural Networks C∆° b·∫£n

**Ki·∫øn tr√∫c:**
```
Input Layer ‚Üí Hidden Layers ‚Üí Output Layer
```

**Forward Propagation:**
```
z = W¬∑x + b
a = activation(z)
```

**Backpropagation:**
- T√≠nh gradient c·ªßa loss theo weights
- Update weights: W = W - learning_rate * gradient

#### 2. PyTorch Basics

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 1. Tensors (gi·ªëng NumPy arrays)
x = torch.tensor([1.0, 2.0, 3.0])
y = torch.tensor([[1, 2], [3, 4]])

# Operations
print(x + 2)
print(y.shape)
print(y.T)  # Transpose

# GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = x.to(device)

# Autograd (t·ª± ƒë·ªông t√≠nh gradient)
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward()
print(x.grad)  # dy/dx = 2x = 4
```

#### 3. X√¢y d·ª±ng Neural Network

```python
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# Kh·ªüi t·∫°o
model = SimpleNN(input_size=10, hidden_size=20, output_size=1)
print(model)
```

#### 4. Training Loop

```python
# Chu·∫©n b·ªã d·ªØ li·ªáu
from torch.utils.data import DataLoader, TensorDataset

X_train_tensor = torch.FloatTensor(X_train.values)
y_train_tensor = torch.FloatTensor(y_train.values)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Model, Loss, Optimizer
model = SimpleNN(input_size=10, hidden_size=20, output_size=1)
criterion = nn.BCELoss()  # Binary Cross Entropy
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch_X, batch_y in train_loader:
        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.FloatTensor(X_test.values)
    predictions = model(X_test_tensor)
    predictions = (predictions > 0.5).float()
    accuracy = (predictions == torch.FloatTensor(y_test.values)).float().mean()
    print(f'Test Accuracy: {accuracy:.4f}')
```



#### 5. Activation Functions

```python
import torch.nn.functional as F

# ReLU (Rectified Linear Unit)
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
print(F.relu(x))  # [0, 0, 0, 1, 2]

# Sigmoid (0 to 1)
print(torch.sigmoid(x))  # [0.12, 0.27, 0.5, 0.73, 0.88]

# Tanh (-1 to 1)
print(torch.tanh(x))  # [-0.96, -0.76, 0, 0.76, 0.96]

# Softmax (for multi-class)
logits = torch.tensor([2.0, 1.0, 0.1])
print(F.softmax(logits, dim=0))  # [0.66, 0.24, 0.10]
```

#### 6. Loss Functions

```python
# Binary Classification
criterion = nn.BCELoss()  # Binary Cross Entropy
criterion = nn.BCEWithLogitsLoss()  # BCE + Sigmoid

# Multi-class Classification
criterion = nn.CrossEntropyLoss()  # Softmax + NLL

# Regression
criterion = nn.MSELoss()  # Mean Squared Error
criterion = nn.L1Loss()   # Mean Absolute Error
```

#### 7. Optimizers

```python
# SGD (Stochastic Gradient Descent)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# Adam (Adaptive Moment Estimation) - Most popular
optimizer = optim.Adam(model.parameters(), lr=0.001)

# RMSprop
optimizer = optim.RMSprop(model.parameters(), lr=0.001)

# Learning rate scheduler
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
```

#### 8. Regularization (Tr√°nh overfitting)

```python
# Dropout
class NNWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.dropout = nn.Dropout(0.5)  # Drop 50% neurons
        self.fc2 = nn.Linear(20, 1)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Batch Normalization
class NNWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.bn1 = nn.BatchNorm1d(20)
        self.fc2 = nn.Linear(20, 1)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

# Weight Decay (L2 regularization)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 5-6

**B√†i 1: Binary Classification v·ªõi PyTorch**
```python
# D·ª± ƒëo√°n gi·ªõi t√≠nh t·ª´ features
# Input: hoTen (encoded), ngheNghiep (encoded), doiThuoc
# Output: gioiTinh (0/1)
#
# Y√™u c·∫ßu:
# 1. X√¢y d·ª±ng NN v·ªõi 2 hidden layers
# 2. S·ª≠ d·ª•ng ReLU activation
# 3. Train 100 epochs
# 4. Plot training loss
# 5. ƒê√°nh gi√° accuracy, precision, recall
```

**B√†i 2: Multi-class Classification**
```python
# Ph√¢n lo·∫°i ngh·ªÅ nghi·ªáp (5 classes)
# Input: trinhDoHocVan, doiThuoc, noiSinh
# Output: ngheNghiep (0-4)
#
# Y√™u c·∫ßu:
# 1. One-hot encode output
# 2. S·ª≠ d·ª•ng CrossEntropyLoss
# 3. Th·ª≠ nghi·ªám v·ªõi Dropout
# 4. Learning rate scheduling
# 5. Confusion matrix
```

**B√†i 3: Regression v·ªõi PyTorch**
```python
# D·ª± ƒëo√°n s·ªë con c·ªßa m·ªôt ng∆∞·ªùi
# Input: age, ngheNghiep, trinhDoHocVan
# Output: s·ªë con (continuous)
#
# Y√™u c·∫ßu:
# 1. MSE Loss
# 2. Batch Normalization
# 3. Early stopping
# 4. Plot predicted vs actual
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **PyTorch**
   - Official Tutorial: https://pytorch.org/tutorials/
   - 60 Minute Blitz: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html

2. **Courses**
   - "Deep Learning Specialization" by Andrew Ng (Coursera)
   - "Practical Deep Learning for Coders" by fast.ai

3. **Videos**
   - "PyTorch Tutorial" by Aladdin Persson (YouTube)
   - "Neural Networks from Scratch" by 3Blue1Brown

4. **Books**
   - "Deep Learning with PyTorch" by Eli Stevens
   - "Deep Learning" by Ian Goodfellow (advanced)

---


# TH√ÅNG 2: NATURAL LANGUAGE PROCESSING (NLP)

## üóìÔ∏è TU·∫¶N 7-8: NLP C∆° b·∫£n

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Text Preprocessing

```python
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Download NLTK data
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# 1. Lowercase
text = "Nguy·ªÖn VƒÉn A l√† con c·ªßa Nguy·ªÖn VƒÉn B"
text = text.lower()

# 2. Remove punctuation
text = text.translate(str.maketrans('', '', string.punctuation))

# 3. Tokenization (t√°ch t·ª´)
tokens = word_tokenize(text)
print(tokens)  # ['nguy·ªÖn', 'vƒÉn', 'a', 'l√†', 'con', 'c·ªßa', ...]

# 4. Remove stopwords
stop_words = set(stopwords.words('vietnamese'))
tokens = [w for w in tokens if w not in stop_words]

# 5. Stemming (c·∫Øt t·ª´ v·ªÅ g·ªëc)
stemmer = PorterStemmer()
stemmed = [stemmer.stem(w) for w in tokens]

# 6. Lemmatization (v·ªÅ d·∫°ng g·ªëc c√≥ nghƒ©a)
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(w) for w in tokens]
```

#### 2. Text Representation

**A. Bag of Words (BoW)**

```python
from sklearn.feature_extraction.text import CountVectorizer

texts = [
    "Nguy·ªÖn VƒÉn A l√† n√¥ng d√¢n",
    "Nguy·ªÖn VƒÉn B l√† gi√°o vi√™n",
    "Tr·∫ßn Th·ªã C l√† b√°c sƒ©"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

print(vectorizer.get_feature_names_out())
print(X.toarray())
# [[1, 0, 0, 1, 1, 1, 0, 0]
#  [0, 1, 0, 1, 1, 1, 0, 0]
#  [0, 0, 1, 0, 0, 1, 1, 1]]
```

**B. TF-IDF (Term Frequency - Inverse Document Frequency)**

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

print(X.toarray())
# T·ª´ xu·∫•t hi·ªán nhi·ªÅu trong 1 doc nh∆∞ng √≠t trong c√°c doc kh√°c ‚Üí TF-IDF cao
```

**C. Word Embeddings (Word2Vec)**

```python
from gensim.models import Word2Vec

# Corpus
sentences = [
    ['nguy·ªÖn', 'vƒÉn', 'a', 'n√¥ng', 'd√¢n'],
    ['nguy·ªÖn', 'vƒÉn', 'b', 'gi√°o', 'vi√™n'],
    ['tr·∫ßn', 'th·ªã', 'c', 'b√°c', 'sƒ©']
]

# Train Word2Vec
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Get vector
vector = model.wv['nguy·ªÖn']
print(vector.shape)  # (100,)

# Similar words
similar = model.wv.most_similar('nguy·ªÖn', topn=5)
print(similar)

# Save/Load
model.save("word2vec.model")
model = Word2Vec.load("word2vec.model")
```



#### 3. Text Classification

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Prepare data
questions = [
    "C√≥ bao nhi√™u ng∆∞·ªùi trong gia ph·∫£?",
    "Ai l√† con c·ªßa Nguy·ªÖn VƒÉn A?",
    "Ngh·ªÅ nghi·ªáp c·ªßa Tr·∫ßn VƒÉn B l√† g√¨?",
    "Nguy·ªÖn VƒÉn C sinh nƒÉm n√†o?"
]

labels = ['count', 'relationship', 'occupation', 'birth_year']

# Vectorize
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(questions)

# Train classifier
classifier = MultinomialNB()
classifier.fit(X, labels)

# Predict
new_question = "C√≥ bao nhi√™u ng∆∞·ªùi l√†m n√¥ng d√¢n?"
X_new = vectorizer.transform([new_question])
prediction = classifier.predict(X_new)
print(prediction)  # ['count']
```

#### 4. Named Entity Recognition (NER)

```python
import spacy

# Load model
nlp = spacy.load("en_core_web_sm")

text = "Nguy·ªÖn VƒÉn A sinh nƒÉm 1950 t·∫°i H√† N·ªôi"
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(f"{ent.text} - {ent.label_}")
# Nguy·ªÖn VƒÉn A - PERSON
# 1950 - DATE
# H√† N·ªôi - GPE (Geo-Political Entity)
```

#### 5. Sequence Models (RNN, LSTM)

```python
import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # hidden: (1, batch_size, hidden_dim)
        output = self.fc(hidden.squeeze(0))
        return output

# Example usage
vocab_size = 10000
embedding_dim = 100
hidden_dim = 256
output_dim = 5  # 5 classes

model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 7-8

**B√†i 1: Text Classification cho c√¢u h·ªèi**
```python
# Ph√¢n lo·∫°i c√¢u h·ªèi v·ªÅ gia ph·∫£ th√†nh c√°c lo·∫°i:
# - count: ƒê·∫øm s·ªë l∆∞·ª£ng
# - relationship: Quan h·ªá gia ƒë√¨nh
# - personal_info: Th√¥ng tin c√° nh√¢n
# - occupation: Ngh·ªÅ nghi·ªáp
# - location: ƒê·ªãa ƒëi·ªÉm
#
# Y√™u c·∫ßu:
# 1. T·∫°o dataset 100 c√¢u h·ªèi m·∫´u
# 2. TF-IDF vectorization
# 3. Train Naive Bayes, Logistic Regression, SVM
# 4. So s√°nh accuracy
# 5. Test v·ªõi c√¢u h·ªèi m·ªõi
```

**B√†i 2: Named Entity Recognition**
```python
# Extract th√¥ng tin t·ª´ c√¢u h·ªèi:
# - T√™n ng∆∞·ªùi (PERSON)
# - NƒÉm (DATE)
# - ƒê·ªãa ƒëi·ªÉm (LOCATION)
#
# V√≠ d·ª•: "Nguy·ªÖn VƒÉn A sinh nƒÉm 1950 t·∫°i H√† N·ªôi"
# ‚Üí PERSON: Nguy·ªÖn VƒÉn A, DATE: 1950, LOCATION: H√† N·ªôi
#
# Y√™u c·∫ßu:
# 1. S·ª≠ d·ª•ng spaCy ho·∫∑c regex
# 2. X·ª≠ l√Ω 50 c√¢u
# 3. T√≠nh precision, recall
```

**B√†i 3: Word Embeddings**
```python
# Train Word2Vec tr√™n d·ªØ li·ªáu gia ph·∫£
# 1. Load t·∫•t c·∫£ text t·ª´ database (hoTen, ngheNghiep, noiSinh, etc.)
# 2. Preprocessing
# 3. Train Word2Vec
# 4. T√¨m t·ª´ t∆∞∆°ng t·ª±
# 5. Visualize embeddings v·ªõi t-SNE
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **NLTK**
   - Official Book: https://www.nltk.org/book/
   - Tutorial: Natural Language Processing with Python

2. **spaCy**
   - Official Tutorial: https://spacy.io/usage/spacy-101

3. **Courses**
   - "Natural Language Processing Specialization" by DeepLearning.AI (Coursera)
   - "NLP with Python" by DataCamp

4. **Videos**
   - "NLP Tutorial" by Krish Naik (YouTube)
   - "Stanford CS224N: NLP with Deep Learning"

---


## üóìÔ∏è TU·∫¶N 9-10: Transformers & Hugging Face

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Transformer Architecture

**Ki·∫øn tr√∫c:**
```
Input Text
    ‚Üì
Tokenization
    ‚Üì
Embedding + Positional Encoding
    ‚Üì
Multi-Head Self-Attention
    ‚Üì
Feed Forward Network
    ‚Üì
Output
```

**Self-Attention:**
- Model h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá gi·ªØa c√°c t·ª´
- V√≠ d·ª•: "Nguy·ªÖn VƒÉn A l√† con c·ªßa Nguy·ªÖn VƒÉn B"
  - "con" attention cao v·ªõi "Nguy·ªÖn VƒÉn A" v√† "Nguy·ªÖn VƒÉn B"

**C√°c model Transformer ph·ªï bi·∫øn:**
- **BERT**: Bidirectional (ƒë·ªçc 2 chi·ªÅu)
- **GPT**: Autoregressive (ƒë·ªçc tr√°i ‚Üí ph·∫£i)
- **T5**: Text-to-Text (input text ‚Üí output text)
- **Qwen**: Code-focused (d·ª± √°n ƒëang d√πng)

#### 2. Hugging Face Transformers

**C√†i ƒë·∫∑t:**
```bash
pip install transformers torch
```

**3 th√†nh ph·∫ßn ch√≠nh:**

**A. Tokenizer**

```python
from transformers import AutoTokenizer

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-7B-Instruct")

# Encode
text = "C√≥ bao nhi√™u ng∆∞·ªùi trong gia ph·∫£?"
inputs = tokenizer(text, return_tensors="pt")

print(inputs)
# {
#   'input_ids': tensor([[123, 456, 789, ...]]),
#   'attention_mask': tensor([[1, 1, 1, ...]])
# }

# Decode
decoded = tokenizer.decode(inputs['input_ids'][0])
print(decoded)

# Batch encoding
texts = ["C√¢u 1", "C√¢u 2", "C√¢u 3"]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
```

**B. Model**

```python
from transformers import AutoModelForCausalLM
import torch

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    cache_dir="./models",
    torch_dtype=torch.float16,  # Gi·∫£m memory
    device_map="auto"           # Auto GPU allocation
)

# Generate text
inputs = tokenizer("Vi·∫øt code Python ƒë·ªÉ", return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9
)

generated_text = tokenizer.decode(outputs[0])
print(generated_text)
```

**C. Pipeline (API ƒë∆°n gi·∫£n)**

```python
from transformers import pipeline

# Text generation
generator = pipeline("text-generation", model="gpt2")
result = generator("Hello, I am", max_length=30)
print(result[0]['generated_text'])

# Sentiment analysis
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.99}]

# Question answering
qa = pipeline("question-answering")
context = "Nguy·ªÖn VƒÉn A sinh nƒÉm 1950 t·∫°i H√† N·ªôi"
question = "Nguy·ªÖn VƒÉn A sinh nƒÉm n√†o?"
result = qa(question=question, context=context)
print(result)  # {'answer': '1950', 'score': 0.95}

# Translation
translator = pipeline("translation_en_to_fr")
result = translator("Hello, how are you?")
print(result)  # [{'translation_text': 'Bonjour, comment allez-vous?'}]
```



#### 3. Generation Parameters

```python
outputs = model.generate(
    input_ids,
    max_new_tokens=512,        # S·ªë tokens t·ªëi ƒëa sinh ra
    max_length=1024,           # T·ªïng length (input + output)
    
    # Sampling strategies
    do_sample=True,            # B·∫≠t sampling (random)
    temperature=0.7,           # 0=deterministic, 1=creative
    top_k=50,                  # Ch·ªâ x√©t 50 tokens c√≥ prob cao nh·∫•t
    top_p=0.9,                 # Nucleus sampling (top 90% cumulative prob)
    
    # Beam search
    num_beams=4,               # Beam search (t√¨m sequence t·ªët nh·∫•t)
    early_stopping=True,
    
    # Penalties
    repetition_penalty=1.2,    # Ph·∫°t l·∫∑p l·∫°i
    length_penalty=1.0,        # Khuy·∫øn kh√≠ch/ph·∫°t ƒë·ªô d√†i
    
    # Special tokens
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id
)
```

#### 4. ·ª®ng d·ª•ng trong d·ª± √°n Text-to-SQL

**File: `ai-service/model_loader.py`**

```python
class ModelLoader:
    _instance = None
    _model = None
    _tokenizer = None
    
    def load_model(self):
        # Singleton pattern - ch·ªâ load 1 l·∫ßn
        if self._model is not None:
            return self._model, self._tokenizer
        
        # Load tokenizer
        self._tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            cache_dir=MODEL_CACHE_DIR,
            trust_remote_code=True
        )
        
        # Load model
        if DEVICE == "cuda" and torch.cuda.is_available():
            self._model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                cache_dir=MODEL_CACHE_DIR,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            self._model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                cache_dir=MODEL_CACHE_DIR,
                trust_remote_code=True
            )
        
        return self._model, self._tokenizer
```

**File: `ai-service/prompt_builder.py`**

```python
class PromptBuilder:
    def build_prompt(self, question):
        # Few-shot learning
        examples = "\n".join([
            f"Question: {ex['question']}\nSQL: {ex['sql']}\n"
            for ex in FEW_SHOT_EXAMPLES
        ])
        
        prompt = f"""You are a SQL expert. Convert Vietnamese questions to SQL queries.

Database Schema:
{DATABASE_SCHEMA}

Examples:
{examples}

Question: {question}
SQL:"""
        
        return prompt
```

**File: `ai-service/sql_generator.py`**

```python
class SQLGenerator:
    def generate_sql(self, question):
        # 1. Build prompt
        prompt = self.prompt_builder.build_prompt(question)
        
        # 2. Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt")
        device = next(self.model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # 3. Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.1,      # Low = more deterministic
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # 4. Decode
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # 5. Extract SQL
        sql = self._extract_sql(generated_text, prompt)
        
        return sql
```



### üéØ B√ÄI T·∫¨P TU·∫¶N 9-10

**B√†i 1: Text Generation v·ªõi GPT-2**
```python
# Sinh vƒÉn b·∫£n ti·∫øp theo
# Input: "Nguy·ªÖn VƒÉn A l√†"
# Output: "Nguy·ªÖn VƒÉn A l√† n√¥ng d√¢n, sinh nƒÉm 1950..."
#
# Y√™u c·∫ßu:
# 1. Load GPT-2 model
# 2. Generate v·ªõi temperature kh√°c nhau (0.1, 0.5, 1.0)
# 3. So s√°nh k·∫øt qu·∫£
# 4. Th·ª≠ top_k v√† top_p
```

**B√†i 2: Question Answering**
```python
# Tr·∫£ l·ªùi c√¢u h·ªèi t·ª´ context
# Context: Th√¥ng tin t·ª´ database (tieuSu c·ªßa th√†nh vi√™n)
# Question: "Nguy·ªÖn VƒÉn A l√†m ngh·ªÅ g√¨?"
#
# Y√™u c·∫ßu:
# 1. S·ª≠ d·ª•ng pipeline("question-answering")
# 2. Test v·ªõi 20 c√¢u h·ªèi
# 3. ƒê√°nh gi√° accuracy
```

**B√†i 3: Text-to-SQL c∆° b·∫£n**
```python
# X√¢y d·ª±ng Text-to-SQL ƒë∆°n gi·∫£n
# Input: "C√≥ bao nhi√™u ng∆∞·ªùi?"
# Output: "SELECT COUNT(*) FROM thanhvien"
#
# Y√™u c·∫ßu:
# 1. T·∫°o 20 examples (few-shot)
# 2. Build prompt template
# 3. Generate SQL v·ªõi Qwen/GPT
# 4. Validate SQL syntax
# 5. Test accuracy
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **Hugging Face**
   - Documentation: https://huggingface.co/docs/transformers
   - Course: https://huggingface.co/learn/nlp-course
   - Models: https://huggingface.co/models

2. **Transformers**
   - "Attention Is All You Need" paper
   - "The Illustrated Transformer" by Jay Alammar
   - "Transformers from Scratch" by Peter Bloem

3. **Videos**
   - "Hugging Face Tutorial" by Abhishek Thakur
   - "Transformers Explained" by StatQuest

---


## üóìÔ∏è TU·∫¶N 11-12: Fine-tuning & Text-to-SQL Project

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Fine-tuning Methods

**A. Full Fine-tuning**
- Train to√†n b·ªô parameters
- C·∫ßn GPU m·∫°nh (7B model = ~28GB VRAM)
- T·ªën th·ªùi gian

**B. LoRA (Low-Rank Adaptation) - Khuy√™n d√πng!**
- Ch·ªâ train m·ªôt ph·∫ßn nh·ªè parameters
- Gi·∫£m 90% memory
- Kh√¥ng l√†m h·ªèng model g·ªëc

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

# 1. Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. Configure LoRA
lora_config = LoraConfig(
    r=16,                           # Rank (8, 16, 32)
    lora_alpha=32,                  # Scaling factor
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# 3. Apply LoRA
model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()
# trainable params: 4M || all params: 7B || trainable%: 0.057%
```

#### 2. Prepare Training Data

```python
from datasets import Dataset

# Training data
train_data = [
    {
        "question": "C√≥ bao nhi√™u ng∆∞·ªùi trong gia ph·∫£?",
        "sql": "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ? AND active_flag = 1"
    },
    {
        "question": "Ai l√† con c·ªßa Nguy·ªÖn VƒÉn A?",
        "sql": "SELECT hoTen FROM thanhvien WHERE chaId = (SELECT thanhVienId FROM thanhvien WHERE hoTen = 'Nguy·ªÖn VƒÉn A' AND dongHoId = ?) AND dongHoId = ?"
    },
    # ... more examples
]

# Create dataset
dataset = Dataset.from_list(train_data)

# Tokenize
def tokenize_function(examples):
    prompts = [build_prompt(q) for q in examples['question']]
    targets = examples['sql']
    
    # Tokenize input
    model_inputs = tokenizer(prompts, max_length=512, truncation=True, padding="max_length")
    
    # Tokenize output
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs

tokenized_dataset = dataset.map(tokenize_function, batched=True)
```

#### 3. Training v·ªõi Trainer API

```python
from transformers import Trainer, TrainingArguments

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,                      # Mixed precision
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="steps",
    eval_steps=100,
    save_total_limit=3,
    warmup_steps=100,
    weight_decay=0.01,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# Train
trainer.train()

# Save
model.save_pretrained("./fine-tuned-model")
tokenizer.save_pretrained("./fine-tuned-model")
```



#### 4. Few-shot Learning (Kh√¥ng c·∫ßn fine-tune)

**∆Øu ƒëi·ªÉm:**
- Kh√¥ng c·∫ßn train
- Nhanh, d·ªÖ implement
- D√πng ƒë∆∞·ª£c ngay

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Accuracy th·∫•p h∆°n fine-tuning
- Ph·ª• thu·ªôc v√†o examples

```python
# File: ai-service/config.py
FEW_SHOT_EXAMPLES = [
    {"question": "C√≥ bao nhi√™u ng∆∞·ªùi?", "sql": "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ?"},
    {"question": "Ai l√† con c·ªßa X?", "sql": "SELECT hoTen FROM thanhvien WHERE chaId = (SELECT thanhVienId FROM thanhvien WHERE hoTen = 'X')"},
    # ... 10-20 examples
]

def build_prompt(question):
    examples_text = "\n".join([
        f"Q: {ex['question']}\nSQL: {ex['sql']}\n"
        for ex in FEW_SHOT_EXAMPLES
    ])
    
    prompt = f"""Convert Vietnamese question to SQL.

Database Schema:
{DATABASE_SCHEMA}

Examples:
{examples_text}

Q: {question}
SQL:"""
    
    return prompt
```

#### 5. Evaluation Metrics

```python
def evaluate_sql_generation(predictions, ground_truths):
    """
    ƒê√°nh gi√° Text-to-SQL
    """
    exact_match = 0
    execution_accuracy = 0
    
    for pred, gt in zip(predictions, ground_truths):
        # 1. Exact Match
        if normalize_sql(pred) == normalize_sql(gt):
            exact_match += 1
        
        # 2. Execution Accuracy (ch·∫°y SQL v√† so s√°nh k·∫øt qu·∫£)
        try:
            pred_result = execute_sql(pred)
            gt_result = execute_sql(gt)
            if pred_result == gt_result:
                execution_accuracy += 1
        except:
            pass
    
    return {
        'exact_match': exact_match / len(predictions),
        'execution_accuracy': execution_accuracy / len(predictions)
    }

def normalize_sql(sql):
    """Chu·∫©n h√≥a SQL ƒë·ªÉ so s√°nh"""
    sql = sql.lower().strip()
    sql = re.sub(r'\s+', ' ', sql)
    return sql
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 11-12 (D·ª∞ √ÅN CH√çNH)

**D·ª± √°n: Ho√†n thi·ªán Text-to-SQL cho Family Tree**

**Phase 1: Data Preparation (2 ng√†y)**
```python
# 1. T·∫°o dataset 100 c√¢u h·ªèi - SQL pairs
# 2. Chia train/val/test (70/15/15)
# 3. Ph√¢n t√≠ch distribution (lo·∫°i c√¢u h·ªèi)
# 4. Validate SQL syntax
```

**Phase 2: Few-shot Implementation (3 ng√†y)**
```python
# 1. Implement prompt builder
# 2. Ch·ªçn 15 examples t·ªët nh·∫•t
# 3. Test v·ªõi validation set
# 4. T√≠nh exact match & execution accuracy
# 5. Error analysis
```

**Phase 3: Fine-tuning v·ªõi LoRA (4 ng√†y)**
```python
# 1. Setup LoRA config
# 2. Prepare training data
# 3. Train 3 epochs
# 4. Evaluate tr√™n test set
# 5. So s√°nh v·ªõi few-shot
```

**Phase 4: Integration & Deployment (5 ng√†y)**
```python
# 1. T√≠ch h·ª£p v√†o FastAPI service
# 2. Add caching
# 3. Error handling
# 4. Logging
# 5. Frontend integration
# 6. Testing end-to-end
```

**Deliverables:**
- [ ] Dataset 100 c√¢u h·ªèi
- [ ] Few-shot implementation
- [ ] Fine-tuned model (optional)
- [ ] FastAPI service
- [ ] Frontend UI
- [ ] Documentation
- [ ] Test results report

### üìñ T√†i li·ªáu tham kh·∫£o

1. **Fine-tuning**
   - Hugging Face Fine-tuning Guide: https://huggingface.co/docs/transformers/training
   - PEFT Documentation: https://huggingface.co/docs/peft

2. **Text-to-SQL**
   - Spider Dataset: https://yale-lily.github.io/spider
   - WikiSQL: https://github.com/salesforce/WikiSQL
   - "Text-to-SQL" papers on arXiv

3. **Videos**
   - "Fine-tuning LLMs" by Weights & Biases
   - "LoRA Explained" by Efficient NLP

---


# TH√ÅNG 3: COMPUTER VISION & ADVANCED TOPICS

## üóìÔ∏è TU·∫¶N 13-14: Computer Vision C∆° b·∫£n

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Image Basics

```python
import cv2
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img = cv2.imread('photo.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Image properties
print(img.shape)  # (height, width, channels)
print(img.dtype)  # uint8 (0-255)

# Display
plt.imshow(img_rgb)
plt.show()

# Resize
img_resized = cv2.resize(img, (224, 224))

# Crop
img_cropped = img[100:300, 100:300]

# Grayscale
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Blur
img_blur = cv2.GaussianBlur(img, (5, 5), 0)

# Edge detection
edges = cv2.Canny(img_gray, 100, 200)
```

#### 2. Image Preprocessing

```python
from torchvision import transforms
from PIL import Image

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])

# Apply
img = Image.open('photo.jpg')
img_tensor = transform(img)
print(img_tensor.shape)  # (3, 224, 224)
```

#### 3. Convolutional Neural Networks (CNN)

**Ki·∫øn tr√∫c CNN:**
```
Input Image (224x224x3)
    ‚Üì
Conv Layer + ReLU (filters extract features)
    ‚Üì
Pooling Layer (reduce size)
    ‚Üì
Conv Layer + ReLU
    ‚Üì
Pooling Layer
    ‚Üì
Flatten
    ‚Üì
Fully Connected Layer
    ‚Üì
Output (classes)
```

**Implement CNN v·ªõi PyTorch:**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # Pooling
        self.pool = nn.MaxPool2d(2, 2)
        
        # Fully connected
        self.fc1 = nn.Linear(128 * 28 * 28, 512)
        self.fc2 = nn.Linear(512, num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # Conv block 1
        x = self.pool(F.relu(self.conv1(x)))  # 224 -> 112
        
        # Conv block 2
        x = self.pool(F.relu(self.conv2(x)))  # 112 -> 56
        
        # Conv block 3
        x = self.pool(F.relu(self.conv3(x)))  # 56 -> 28
        
        # Flatten
        x = x.view(-1, 128 * 28 * 28)
        
        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# Create model
model = SimpleCNN(num_classes=10)
print(model)
```



#### 4. Training CNN

```python
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Prepare data
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.ImageFolder('data/train', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Model, loss, optimizer
model = SimpleCNN(num_classes=len(train_dataset.classes))
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        
        # Forward
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%')
```

#### 5. Transfer Learning (Khuy√™n d√πng!)

**T·∫°i sao d√πng Transfer Learning?**
- Model ƒë√£ train tr√™n ImageNet (1M images)
- H·ªçc ƒë∆∞·ª£c features t·ªïng qu√°t (edges, textures, shapes)
- Ch·ªâ c·∫ßn fine-tune cho task c·ª• th·ªÉ
- Ti·∫øt ki·ªám th·ªùi gian v√† data

```python
from torchvision import models

# Load pre-trained ResNet
model = models.resnet50(pretrained=True)

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

# Replace last layer
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Only train last layer
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)

# Or fine-tune all layers with small lr
for param in model.parameters():
    param.requires_grad = True
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
```

**Popular pre-trained models:**
```python
# ResNet (Residual Networks)
model = models.resnet18(pretrained=True)
model = models.resnet50(pretrained=True)

# VGG
model = models.vgg16(pretrained=True)

# EfficientNet
model = models.efficientnet_b0(pretrained=True)

# Vision Transformer (ViT)
from transformers import ViTForImageClassification
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 13-14

**B√†i 1: Image Classification - Ph√¢n lo·∫°i ·∫£nh th√†nh vi√™n**
```python
# Ph√¢n lo·∫°i ·∫£nh th√†nh vi√™n theo:
# - Nam/N·ªØ
# - ƒê·ªô tu·ªïi (tr·∫ª em, thanh ni√™n, trung ni√™n, cao tu·ªïi)
#
# Y√™u c·∫ßu:
# 1. T·∫°o dataset t·ª´ ·∫£nh trong database
# 2. Train CNN t·ª´ scratch
# 3. Transfer learning v·ªõi ResNet
# 4. So s√°nh accuracy
# 5. Confusion matrix
```

**B√†i 2: Face Detection**
```python
# Detect khu√¥n m·∫∑t trong ·∫£nh gia ƒë√¨nh
# S·ª≠ d·ª•ng OpenCV Haar Cascade ho·∫∑c MTCNN
#
# Y√™u c·∫ßu:
# 1. Load ·∫£nh t·ª´ database
# 2. Detect faces
# 3. Crop v√† save faces
# 4. Count s·ªë ng∆∞·ªùi trong ·∫£nh
```

**B√†i 3: Image Augmentation**
```python
# TƒÉng c∆∞·ªùng d·ªØ li·ªáu ·∫£nh
# 1. Random flip, rotation, crop
# 2. Color jitter
# 3. Gaussian blur
# 4. T·∫°o 5 versions cho m·ªói ·∫£nh
# 5. Visualize augmentations
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **OpenCV**
   - Official Tutorial: https://docs.opencv.org/master/d9/df8/tutorial_root.html
   - "OpenCV Python Tutorial" by freeCodeCamp

2. **PyTorch Vision**
   - torchvision docs: https://pytorch.org/vision/stable/index.html
   - Transfer Learning Tutorial: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html

3. **Courses**
   - "Convolutional Neural Networks" by Andrew Ng (Coursera)
   - "Deep Learning for Computer Vision" by Stanford (CS231n)

4. **Books**
   - "Deep Learning for Computer Vision" by Rajalingappaa Shanmugamani

---


## üóìÔ∏è TU·∫¶N 15-16: Object Detection & Advanced CV

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Object Detection

**Kh√°c bi·ªát v·ªõi Classification:**
- **Classification**: "ƒê√¢y l√† con m√®o" (1 label)
- **Object Detection**: "C√≥ 2 con m√®o ·ªü v·ªã tr√≠ (x1,y1,x2,y2) v√† (x3,y3,x4,y4)" (multiple objects + bounding boxes)

**Popular models:**
- **YOLO (You Only Look Once)**: Fast, real-time
- **Faster R-CNN**: Accurate but slower
- **SSD (Single Shot Detector)**: Balance speed/accuracy

#### 2. YOLO v·ªõi Ultralytics

```python
from ultralytics import YOLO
import cv2

# Load pre-trained model
model = YOLO('yolov8n.pt')  # nano (fastest)
# model = YOLO('yolov8s.pt')  # small
# model = YOLO('yolov8m.pt')  # medium
# model = YOLO('yolov8l.pt')  # large

# Detect objects
results = model('image.jpg')

# Process results
for result in results:
    boxes = result.boxes
    for box in boxes:
        # Bounding box
        x1, y1, x2, y2 = box.xyxy[0]
        
        # Confidence
        conf = box.conf[0]
        
        # Class
        cls = box.cls[0]
        class_name = model.names[int(cls)]
        
        print(f"{class_name}: {conf:.2f} at ({x1},{y1},{x2},{y2})")

# Visualize
annotated = results[0].plot()
cv2.imshow('Detection', annotated)
cv2.waitKey(0)
```

#### 3. Fine-tune YOLO cho custom dataset

```python
from ultralytics import YOLO

# Prepare dataset structure:
# dataset/
#   ‚îú‚îÄ‚îÄ images/
#   ‚îÇ   ‚îú‚îÄ‚îÄ train/
#   ‚îÇ   ‚îî‚îÄ‚îÄ val/
#   ‚îî‚îÄ‚îÄ labels/
#       ‚îú‚îÄ‚îÄ train/
#       ‚îî‚îÄ‚îÄ val/

# Create data.yaml
"""
train: dataset/images/train
val: dataset/images/val
nc: 2  # number of classes
names: ['person', 'face']
"""

# Load model
model = YOLO('yolov8n.pt')

# Train
results = model.train(
    data='data.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    name='family_detection'
)

# Validate
metrics = model.val()

# Predict
results = model.predict('test_image.jpg')
```

#### 4. Face Recognition

```python
import face_recognition
import cv2

# Load image
image = face_recognition.load_image_file("family_photo.jpg")

# Find faces
face_locations = face_recognition.face_locations(image)
face_encodings = face_recognition.face_encodings(image, face_locations)

print(f"Found {len(face_locations)} faces")

# Compare faces
known_image = face_recognition.load_image_file("nguyen_van_a.jpg")
known_encoding = face_recognition.face_encodings(known_image)[0]

for face_encoding in face_encodings:
    matches = face_recognition.compare_faces([known_encoding], face_encoding)
    if matches[0]:
        print("Found Nguy·ªÖn VƒÉn A!")
```

#### 5. Image Segmentation

```python
from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor
from PIL import Image

# Load model
processor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")

# Process image
image = Image.open("photo.jpg")
inputs = processor(images=image, return_tensors="pt")

# Predict
outputs = model(**inputs)
logits = outputs.logits

# Get segmentation map
predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
```



### üéØ B√ÄI T·∫¨P TU·∫¶N 15-16

**B√†i 1: Face Detection trong ·∫£nh gia ƒë√¨nh**
```python
# Detect t·∫•t c·∫£ khu√¥n m·∫∑t trong ·∫£nh gia ƒë√¨nh
# 1. Load ·∫£nh t·ª´ database
# 2. Detect faces v·ªõi YOLO ho·∫∑c MTCNN
# 3. Crop v√† save t·ª´ng face
# 4. T·∫°o bounding boxes
# 5. Count s·ªë ng∆∞·ªùi
```

**B√†i 2: Face Recognition**
```python
# Nh·∫≠n di·ªán th√†nh vi√™n trong ·∫£nh
# 1. T·∫°o face encodings cho m·ªói th√†nh vi√™n
# 2. Load ·∫£nh m·ªõi
# 3. Detect v√† recognize faces
# 4. Label t√™n ng∆∞·ªùi
# 5. T√≠nh accuracy
```

**B√†i 3: Auto-tag ·∫£nh**
```python
# T·ª± ƒë·ªông g·∫Øn tag cho ·∫£nh
# Input: ·∫¢nh gia ƒë√¨nh
# Output: Tags (s·ªë ng∆∞·ªùi, s·ª± ki·ªán, ƒë·ªãa ƒëi·ªÉm)
#
# 1. Object detection (ng∆∞·ªùi, v·∫≠t)
# 2. Scene classification (trong nh√†/ngo√†i tr·ªùi)
# 3. Face recognition (t√™n ng∆∞·ªùi)
# 4. Save tags v√†o database
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **YOLO**
   - Ultralytics Docs: https://docs.ultralytics.com/
   - YOLOv8 Tutorial: https://github.com/ultralytics/ultralytics

2. **Face Recognition**
   - face_recognition library: https://github.com/ageitgey/face_recognition
   - DeepFace: https://github.com/serengil/deepface

3. **Courses**
   - "Object Detection" by Andrew Ng
   - "Advanced Computer Vision" by Stanford

---

## üóìÔ∏è TU·∫¶N 17-18: Deployment & Production

### üìö Ki·∫øn th·ª©c c·∫ßn h·ªçc

#### 1. Model Optimization

**A. Quantization (Gi·∫£m k√≠ch th∆∞·ªõc model)**

```python
import torch

# Dynamic Quantization
model_quantized = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# Size comparison
torch.save(model.state_dict(), 'model.pth')
torch.save(model_quantized.state_dict(), 'model_quantized.pth')
# Original: 500MB ‚Üí Quantized: 125MB
```

**B. Pruning (Lo·∫°i b·ªè weights kh√¥ng quan tr·ªçng)**

```python
import torch.nn.utils.prune as prune

# Prune 30% of weights
prune.l1_unstructured(model.fc1, name='weight', amount=0.3)

# Remove pruning
prune.remove(model.fc1, 'weight')
```

**C. ONNX Export (T∆∞∆°ng th√≠ch nhi·ªÅu framework)**

```python
import torch.onnx

# Export to ONNX
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)

# Load ONNX
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
outputs = session.run(None, {'input': input_data})
```

#### 2. FastAPI Deployment (ƒê√£ implement trong d·ª± √°n)

```python
from fastapi import FastAPI, File, UploadFile
from PIL import Image
import io

app = FastAPI()

# Load model once
model = load_model()

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    # Read image
    image_bytes = await file.read()
    image = Image.open(io.BytesIO(image_bytes))
    
    # Preprocess
    image_tensor = transform(image).unsqueeze(0)
    
    # Predict
    with torch.no_grad():
        output = model(image_tensor)
        prediction = torch.argmax(output, dim=1).item()
    
    return {"prediction": prediction}

@app.get("/health")
async def health():
    return {"status": "ok"}
```



#### 3. Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY . .

# Expose port
EXPOSE 7000

# Run
CMD ["python", "main.py"]
```

```bash
# Build
docker build -t ai-service .

# Run
docker run -p 7000:7000 ai-service
```

#### 4. Caching & Performance

```python
from functools import lru_cache
import redis

# In-memory cache
@lru_cache(maxsize=1000)
def get_prediction(image_hash):
    return model.predict(image_hash)

# Redis cache
redis_client = redis.Redis(host='localhost', port=6379)

def predict_with_cache(image):
    # Generate hash
    image_hash = hashlib.md5(image.tobytes()).hexdigest()
    
    # Check cache
    cached = redis_client.get(image_hash)
    if cached:
        return json.loads(cached)
    
    # Predict
    result = model.predict(image)
    
    # Save to cache
    redis_client.setex(image_hash, 3600, json.dumps(result))
    
    return result
```

#### 5. Monitoring & Logging

```python
import logging
from prometheus_client import Counter, Histogram
import time

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Metrics
prediction_counter = Counter('predictions_total', 'Total predictions')
prediction_duration = Histogram('prediction_duration_seconds', 'Prediction duration')

@app.post("/predict")
async def predict(file: UploadFile):
    start_time = time.time()
    
    try:
        # Predict
        result = model.predict(image)
        
        # Metrics
        prediction_counter.inc()
        prediction_duration.observe(time.time() - start_time)
        
        # Log
        logger.info(f"Prediction: {result}, Duration: {time.time() - start_time:.2f}s")
        
        return result
    
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        raise
```

### üéØ B√ÄI T·∫¨P TU·∫¶N 17-18 (D·ª∞ √ÅN CU·ªêI)

**D·ª± √°n: Face Recognition Service cho Family Tree**

**Requirements:**
1. Detect faces trong ·∫£nh upload
2. Recognize th√†nh vi√™n
3. Auto-tag ·∫£nh
4. RESTful API
5. Caching
6. Docker deployment

**Phase 1: Model Development (3 ng√†y)**
```python
# 1. Collect face images t·ª´ database
# 2. Train face recognition model
# 3. Evaluate accuracy
# 4. Optimize model (quantization)
```

**Phase 2: API Development (3 ng√†y)**
```python
# 1. FastAPI endpoints:
#    - POST /detect-faces
#    - POST /recognize-faces
#    - POST /auto-tag
# 2. Image upload handling
# 3. Response formatting
# 4. Error handling
```

**Phase 3: Optimization (2 ng√†y)**
```python
# 1. Redis caching
# 2. Batch processing
# 3. Async processing
# 4. Load testing
```

**Phase 4: Deployment (3 ng√†y)**
```python
# 1. Dockerfile
# 2. Docker Compose (AI service + Redis)
# 3. Integration v·ªõi backend
# 4. Frontend UI
# 5. Testing
```

**Phase 5: Documentation (3 ng√†y)**
```python
# 1. API documentation (Swagger)
# 2. Setup guide
# 3. Architecture diagram
# 4. Performance report
```

### üìñ T√†i li·ªáu tham kh·∫£o

1. **Deployment**
   - FastAPI Docs: https://fastapi.tiangolo.com/
   - Docker Tutorial: https://docs.docker.com/get-started/

2. **Optimization**
   - PyTorch Quantization: https://pytorch.org/docs/stable/quantization.html
   - ONNX Runtime: https://onnxruntime.ai/

3. **Production ML**
   - "Designing Machine Learning Systems" by Chip Huyen
   - "Machine Learning Engineering" by Andriy Burkov

---


# KI·∫æN TR√öC AI TRONG D·ª∞ √ÅN FAMILY TREE

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FRONTEND (Next.js)                       ‚îÇ
‚îÇ                         Port 3000                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  AI Chat UI  ‚îÇ  ‚îÇ  Face Upload ‚îÇ  ‚îÇ  Auto-tag UI ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      API GATEWAY (Express)                       ‚îÇ
‚îÇ                         Port 8080                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Authentication, Rate Limiting, CORS                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      BACKEND (Node.js)                           ‚îÇ
‚îÇ                         Port 6001                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ AI Query     ‚îÇ  ‚îÇ Face Recog   ‚îÇ  ‚îÇ Member CRUD  ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ Service      ‚îÇ  ‚îÇ Service      ‚îÇ  ‚îÇ Service      ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   AI SERVICE     ‚îÇ  ‚îÇ   CV SERVICE     ‚îÇ
‚îÇ   (Python)       ‚îÇ  ‚îÇ   (Python)       ‚îÇ
‚îÇ   Port 7000      ‚îÇ  ‚îÇ   Port 7001      ‚îÇ
‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Qwen Model   ‚îÇ ‚îÇ  ‚îÇ ‚îÇ YOLO Model   ‚îÇ ‚îÇ
‚îÇ ‚îÇ Text-to-SQL  ‚îÇ ‚îÇ  ‚îÇ ‚îÇ Face Recog   ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         DATABASE (MySQL)                 ‚îÇ
‚îÇ         Port 3306                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ thanhvien, quanhe, sukien, etc.   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
project/
‚îú‚îÄ‚îÄ FE/tree/                          # Frontend (Next.js)
‚îÇ   ‚îú‚îÄ‚îÄ app/(full-page)/ai-chat/     # AI Chat UI
‚îÇ   ‚îú‚îÄ‚îÄ service/aiQuery.service.ts   # API calls
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ api-gateway/                      # API Gateway (Express)
‚îÇ   ‚îú‚îÄ‚îÄ config/gateway.config.yml    # Routes config
‚îÇ   ‚îî‚îÄ‚îÄ plugins/                     # Auth plugins
‚îÇ
‚îú‚îÄ‚îÄ myFamilyTree/                     # Backend (Node.js)
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiQueryService.ts    # AI integration
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ thanhVienService.ts  # Member CRUD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ ai-service/                       # AI Service (Python)
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Config & examples
‚îÇ   ‚îú‚îÄ‚îÄ model_loader.py              # Load Hugging Face model
‚îÇ   ‚îú‚îÄ‚îÄ prompt_builder.py            # Build prompts
‚îÇ   ‚îú‚îÄ‚îÄ sql_generator.py             # Generate SQL
‚îÇ   ‚îú‚îÄ‚îÄ query_executor.py            # Execute SQL
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # FastAPI server
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ cv-service/                       # Computer Vision (Future)
‚îÇ   ‚îú‚îÄ‚îÄ face_detection.py
‚îÇ   ‚îú‚îÄ‚îÄ face_recognition.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ
‚îî‚îÄ‚îÄ database/
    ‚îú‚îÄ‚îÄ tree_v26.sql                 # Database schema
    ‚îî‚îÄ‚îÄ relationship_sync_procedures.sql
```



## üîÑ Data Flow - Text-to-SQL

### 1. User asks question

```typescript
// Frontend: FE/tree/app/(full-page)/ai-chat/page.tsx
const handleAsk = async () => {
  const response = await askAIQuestion(question, user.dongHoId);
  setResult(response.data);
};
```

### 2. Frontend ‚Üí Backend

```typescript
// Frontend: FE/tree/service/aiQuery.service.ts
export const askAIQuestion = async (question: string, dongHoId: string) => {
  const response = await api.post('/ai/query', { question, dongHoId });
  return response;
};
```

### 3. Backend ‚Üí AI Service

```typescript
// Backend: myFamilyTree/src/services/aiQueryService.ts
async askQuestion(question: string, dongHoId: string) {
  const response = await axios.post(`${AI_SERVICE_URL}/query`, {
    question,
    dongHoId,
    execute: true
  });
  return response.data;
}
```

### 4. AI Service processes

```python
# AI Service: ai-service/main.py
@app.post("/query")
async def process_query(request: QueryRequest):
    # 1. Generate SQL
    result = sql_generator.generate_sql(request.question)
    sql = result["sql"]
    
    # 2. Execute SQL
    params = [request.dongHoId] * sql.count('?')
    exec_result = query_executor.execute_query(sql, params)
    
    return {
        "sql": sql,
        "data": exec_result["data"],
        "columns": exec_result["columns"]
    }
```

### 5. SQL Generation

```python
# AI Service: ai-service/sql_generator.py
def generate_sql(self, question):
    # 1. Build prompt with few-shot examples
    prompt = self.prompt_builder.build_prompt(question)
    
    # 2. Tokenize
    inputs = self.tokenizer(prompt, return_tensors="pt")
    
    # 3. Generate with Qwen model
    outputs = self.model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.9
    )
    
    # 4. Decode
    generated_text = self.tokenizer.decode(outputs[0])
    
    # 5. Extract SQL
    sql = self._extract_sql(generated_text)
    
    return sql
```

### 6. Execute SQL

```python
# AI Service: ai-service/query_executor.py
def execute_query(self, sql, params):
    connection = mysql.connector.connect(**DB_CONFIG)
    cursor = connection.cursor(dictionary=True)
    
    cursor.execute(sql, params)
    results = cursor.fetchall()
    
    return {
        "data": results,
        "columns": list(results[0].keys()) if results else []
    }
```

### 7. Return to Frontend

```typescript
// Frontend displays results
{result.results.map((row, idx) => (
  <tr key={idx}>
    {result.columns.map(col => (
      <td key={col}>{row[col]}</td>
    ))}
  </tr>
))}
```



## üéì Key Learnings t·ª´ D·ª± √°n

### 1. Hugging Face Model Loading

```python
# Singleton pattern ƒë·ªÉ load model 1 l·∫ßn
class ModelLoader:
    _instance = None
    _model = None
    _tokenizer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelLoader, cls).__new__(cls)
        return cls._instance
```

**Lesson**: Load model n·∫∑ng (7B parameters) ch·ªâ 1 l·∫ßn khi startup, reuse cho m·ªçi requests.

### 2. Few-shot Learning

```python
FEW_SHOT_EXAMPLES = [
    {"question": "C√≥ bao nhi√™u ng∆∞·ªùi?", "sql": "SELECT COUNT(*) FROM thanhvien"},
    {"question": "Ai l√† con c·ªßa X?", "sql": "SELECT hoTen FROM thanhvien WHERE chaId = ..."},
    # ... 10-15 examples
]
```

**Lesson**: Kh√¥ng c·∫ßn fine-tune, ch·ªâ c·∫ßn 10-15 examples t·ªët ƒë√£ c√≥ k·∫øt qu·∫£ kh√°.

### 3. Prompt Engineering

```python
prompt = f"""You are a SQL expert. Convert Vietnamese questions to SQL.

Database Schema:
{DATABASE_SCHEMA}

Examples:
{examples}

Question: {question}
SQL:"""
```

**Lesson**: Prompt structure quan tr·ªçng: Schema ‚Üí Examples ‚Üí Question ‚Üí Output format.

### 4. Temperature Control

```python
outputs = model.generate(
    temperature=0.1,  # Low = deterministic, good for SQL
    top_p=0.9
)
```

**Lesson**: SQL generation c·∫ßn deterministic (temperature th·∫•p), creative writing c·∫ßn temperature cao.

### 5. Microservices Architecture

```
Frontend (3000) ‚Üí Gateway (8080) ‚Üí Backend (6001) ‚Üí AI Service (7000)
```

**Lesson**: T√°ch AI service ri√™ng ‚Üí d·ªÖ scale, deploy, maintain.

---


# T√ÄI NGUY√äN H·ªåC T·∫¨P

## üìö Courses (Khuy√™n d√πng)

### Machine Learning
1. **Machine Learning by Andrew Ng** (Coursera) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Link: https://www.coursera.org/learn/machine-learning
   - Th·ªùi gian: 11 weeks
   - Mi·ªÖn ph√≠ audit

2. **Python for Data Science** (DataCamp)
   - Link: https://www.datacamp.com/tracks/python-programmer
   - Hands-on exercises

### Deep Learning
1. **Deep Learning Specialization** by Andrew Ng (Coursera) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - 5 courses: Neural Networks, Optimization, Structuring ML Projects, CNN, RNN
   - Link: https://www.coursera.org/specializations/deep-learning

2. **Practical Deep Learning for Coders** (fast.ai) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Link: https://course.fast.ai/
   - Top-down approach, code-first
   - Mi·ªÖn ph√≠

### NLP
1. **Natural Language Processing Specialization** (Coursera)
   - Link: https://www.coursera.org/specializations/natural-language-processing

2. **Hugging Face Course** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Link: https://huggingface.co/learn/nlp-course
   - Mi·ªÖn ph√≠, hands-on
   - Transformers, fine-tuning

### Computer Vision
1. **CS231n: Convolutional Neural Networks** (Stanford) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Link: http://cs231n.stanford.edu/
   - Video lectures on YouTube
   - Assignments available

## üìñ Books

### Beginner
1. **"Hands-On Machine Learning"** by Aur√©lien G√©ron ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Scikit-learn, Keras, TensorFlow
   - Practical examples

2. **"Python Machine Learning"** by Sebastian Raschka
   - Comprehensive, code examples

### Intermediate
1. **"Deep Learning with PyTorch"** by Eli Stevens
   - PyTorch fundamentals
   - Computer Vision, NLP examples

2. **"Natural Language Processing with Transformers"** by Lewis Tunstall
   - Hugging Face ecosystem
   - Fine-tuning, deployment

### Advanced
1. **"Deep Learning"** by Ian Goodfellow
   - Theoretical foundations
   - Math-heavy

2. **"Designing Machine Learning Systems"** by Chip Huyen
   - Production ML
   - System design

## üé• YouTube Channels

1. **StatQuest with Josh Starmer** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Gi·∫£i th√≠ch tr·ª±c quan, d·ªÖ hi·ªÉu
   - ML, DL, Statistics

2. **3Blue1Brown**
   - Neural Networks series
   - Math visualization

3. **Sentdex**
   - Python, ML, DL tutorials
   - Hands-on projects

4. **Krish Naik**
   - ML, DL, NLP tutorials
   - End-to-end projects

5. **Yannic Kilcher**
   - Paper reviews
   - Advanced topics

## üåê Websites & Platforms

1. **Kaggle** (https://www.kaggle.com)
   - Competitions
   - Datasets
   - Notebooks
   - Learn section

2. **Papers with Code** (https://paperswithcode.com)
   - Latest research papers
   - Code implementations
   - Benchmarks

3. **Hugging Face** (https://huggingface.co)
   - Models
   - Datasets
   - Spaces (demos)

4. **Google Colab** (https://colab.research.google.com)
   - Free GPU
   - Jupyter notebooks

## üìù Practice Platforms

1. **LeetCode** - Coding practice
2. **HackerRank** - ML/AI challenges
3. **Kaggle Competitions** - Real-world problems
4. **DrivenData** - Social impact competitions

## üîß Tools & Libraries

### Essential
```bash
# Data Science
pip install numpy pandas matplotlib seaborn

# Machine Learning
pip install scikit-learn

# Deep Learning
pip install torch torchvision
pip install tensorflow

# NLP
pip install transformers datasets
pip install nltk spacy

# Computer Vision
pip install opencv-python pillow
pip install ultralytics  # YOLO
```

### Development
```bash
# Jupyter
pip install jupyter notebook

# Experiment tracking
pip install wandb mlflow

# Deployment
pip install fastapi uvicorn
pip install docker
```



---

# üìä LEARNING SCHEDULE - 3 TH√ÅNG

## Th√°ng 1: ML & DL Foundations

| Tu·∫ßn | Ch·ªß ƒë·ªÅ | Th·ªùi gian/ng√†y | B√†i t·∫≠p |
|------|--------|----------------|---------|
| 1-2 | Python, NumPy, Pandas | 2-3h | Ph√¢n t√≠ch Family Tree data |
| 3-4 | Scikit-learn, ML Algorithms | 3-4h | D·ª± ƒëo√°n ngh·ªÅ nghi·ªáp, ph√¢n nh√≥m |
| 5-6 | PyTorch, Neural Networks | 3-4h | Binary/Multi-class classification |

**M·ª•c ti√™u cu·ªëi th√°ng:**
- [ ] Th√†nh th·∫°o NumPy, Pandas
- [ ] Hi·ªÉu 5 thu·∫≠t to√°n ML c∆° b·∫£n
- [ ] X√¢y d·ª±ng ƒë∆∞·ª£c Neural Network v·ªõi PyTorch
- [ ] Ho√†n th√†nh 6 b√†i t·∫≠p

## Th√°ng 2: NLP & Transformers

| Tu·∫ßn | Ch·ªß ƒë·ªÅ | Th·ªùi gian/ng√†y | B√†i t·∫≠p |
|------|--------|----------------|---------|
| 7-8 | NLP basics, Text preprocessing | 2-3h | Text classification, NER |
| 9-10 | Transformers, Hugging Face | 3-4h | Text generation, QA |
| 11-12 | Fine-tuning, Text-to-SQL | 4-5h | **D·ª∞ √ÅN CH√çNH** |

**M·ª•c ti√™u cu·ªëi th√°ng:**
- [ ] Hi·ªÉu Transformer architecture
- [ ] S·ª≠ d·ª•ng ƒë∆∞·ª£c Hugging Face
- [ ] Ho√†n th√†nh Text-to-SQL project
- [ ] Deploy AI service

## Th√°ng 3: Computer Vision & Production

| Tu·∫ßn | Ch·ªß ƒë·ªÅ | Th·ªùi gian/ng√†y | B√†i t·∫≠p |
|------|--------|----------------|---------|
| 13-14 | CNN, Transfer Learning | 3-4h | Image classification |
| 15-16 | Object Detection, Face Recognition | 3-4h | Face detection/recognition |
| 17-18 | Deployment, Optimization | 4-5h | **D·ª∞ √ÅN CU·ªêI** |

**M·ª•c ti√™u cu·ªëi th√°ng:**
- [ ] Hi·ªÉu CNN architecture
- [ ] Implement face recognition
- [ ] Deploy CV service
- [ ] Ho√†n th√†nh 2 d·ª± √°n l·ªõn

---

# üéØ MILESTONES

## Milestone 1: ML Fundamentals (Tu·∫ßn 4)
- ‚úÖ Ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi Pandas
- ‚úÖ Train 3 ML models
- ‚úÖ Evaluate v√† compare models
- ‚úÖ Visualize results

## Milestone 2: Deep Learning (Tu·∫ßn 6)
- ‚úÖ Build Neural Network t·ª´ scratch
- ‚úÖ Train tr√™n GPU
- ‚úÖ Implement regularization
- ‚úÖ Achieve >80% accuracy

## Milestone 3: Text-to-SQL (Tu·∫ßn 12)
- ‚úÖ Dataset 100 c√¢u h·ªèi
- ‚úÖ Few-shot implementation
- ‚úÖ FastAPI service
- ‚úÖ Frontend integration
- ‚úÖ >70% execution accuracy

## Milestone 4: Face Recognition (Tu·∫ßn 18)
- ‚úÖ Face detection model
- ‚úÖ Face recognition system
- ‚úÖ Auto-tagging feature
- ‚úÖ Docker deployment
- ‚úÖ Complete documentation

---

# üí° TIPS H·ªåC T·∫¨P

## 1. H·ªçc theo d·ª± √°n
- ƒê·ª´ng ch·ªâ h·ªçc l√Ω thuy·∫øt
- √Åp d·ª•ng ngay v√†o Family Tree project
- M·ªói concept h·ªçc ‚Üí implement ngay

## 2. Code m·ªói ng√†y
- √çt nh·∫•t 1-2 gi·ªù/ng√†y
- Consistency > Intensity
- GitHub streak

## 3. ƒê·ªçc code ng∆∞·ªùi kh√°c
- Kaggle notebooks
- GitHub repositories
- Hugging Face models

## 4. Tham gia c·ªông ƒë·ªìng
- Reddit: r/MachineLearning, r/learnmachinelearning
- Discord: Hugging Face, fast.ai
- Stack Overflow

## 5. L√†m b√†i t·∫≠p
- Kh√¥ng skip b√†i t·∫≠p
- T·ª± t·∫°o th√™m b√†i t·∫≠p
- Chia s·∫ª code l√™n GitHub

## 6. ƒê·ªçc papers
- B·∫Øt ƒë·∫ßu t·ª´ blog posts
- Sau ƒë√≥ ƒë·ªçc papers
- Implement papers

## 7. Track progress
- Notion/Obsidian ƒë·ªÉ ghi ch√∫
- GitHub ƒë·ªÉ l∆∞u code
- Blog ƒë·ªÉ chia s·∫ª

---

# üöÄ NEXT STEPS

## Sau 3 th√°ng, b·∫°n c√≥ th·ªÉ:

### 1. N√¢ng cao k·ªπ nƒÉng
- [ ] H·ªçc Reinforcement Learning
- [ ] Generative AI (GANs, Diffusion Models)
- [ ] MLOps (Kubernetes, CI/CD)
- [ ] Distributed Training

### 2. Ch·ª©ng ch·ªâ
- [ ] TensorFlow Developer Certificate
- [ ] AWS Machine Learning Specialty
- [ ] Google Professional ML Engineer

### 3. D·ª± √°n m·ªü r·ªông
- [ ] Chatbot v·ªõi RAG (Retrieval Augmented Generation)
- [ ] Recommendation system
- [ ] Time series forecasting
- [ ] Multi-modal AI (text + image)

### 4. ƒê√≥ng g√≥p open source
- [ ] Contribute to Hugging Face
- [ ] Create own models
- [ ] Write tutorials

### 5. Career
- [ ] Build portfolio
- [ ] Apply for ML Engineer positions
- [ ] Freelance AI projects

---

# üìû SUPPORT

N·∫øu g·∫∑p kh√≥ khƒÉn:
1. Google error message
2. Stack Overflow
3. GitHub Issues
4. Discord communities
5. Ask ChatGPT/Claude

**Remember**: Everyone struggles at first. Keep coding! üí™

---

**Created**: January 2026
**Last Updated**: January 2026
**Author**: AI Learning Roadmap for Family Tree Project
**Version**: 1.0

