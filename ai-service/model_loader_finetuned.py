"""
Model Loader cho Fine-tuned Model
Load model ƒë√£ train s·∫µn (nh∆∞ tr√≠ nh·ªõ) - NHANH, kh√¥ng c·∫ßn train l·∫°i
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import os

class FinetunedModelLoader:
    def __init__(self):
        self._model = None
        self._tokenizer = None
        self.base_model_name = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
        self.finetuned_path = "./finetuned_model"
        self.cache_dir = "./models"
        
    def load_model(self):
        """
        Load model ƒë√£ fine-tune
        - L·∫ßn 1: Load base model + adapter (2-3 ph√∫t)
        - L·∫ßn 2+: Load t·ª´ cache (10-20 gi√¢y) ‚ö°
        """
        print("="*60)
        print("üß† LOADING FINE-TUNED MODEL (TR√ç NH·ªö)")
        print("="*60)
        
        # Ki·ªÉm tra c√≥ model ƒë√£ train kh√¥ng
        if not os.path.exists(self.finetuned_path):
            print(f"‚ùå Ch∆∞a c√≥ model fine-tuned t·∫°i: {self.finetuned_path}")
            print("üí° H√£y ch·∫°y: python finetune_lora.py")
            raise FileNotFoundError("Fine-tuned model not found")
        
        print(f"üìÇ Base model: {self.base_model_name}")
        print(f"üìÇ Fine-tuned: {self.finetuned_path}")
        print(f"üìÇ Cache: {self.cache_dir}")
        
        # 1. Load tokenizer
        print("\nüîÑ [1/3] Loading tokenizer...")
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.finetuned_path,
            trust_remote_code=True
        )
        print("‚úÖ Tokenizer loaded")
        
        # 2. Load base model (T·ªêI ∆ØU CHO RAM TH·∫§P - 8GB)
        print("\nüîÑ [2/3] Loading base model...")
        print("‚ö†Ô∏è  RAM th·∫•p - ƒêang t·ªëi ∆∞u...")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            cache_dir=self.cache_dir,
            torch_dtype=torch.float16,  # float16 nh·∫π h∆°n float32
            device_map="cpu",  # Force CPU
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # T·ªëi ∆∞u RAM
            max_memory={0: "6GB", "cpu": "6GB"}  # Gi·ªõi h·∫°n RAM
        )
        print("‚úÖ Base model loaded")
        
        # 3. Load LoRA adapters (TR√ç NH·ªö ƒê√É H·ªåC)
        print("\nüîÑ [3/3] Loading LoRA adapters (tr√≠ nh·ªõ)...")
        self._model = PeftModel.from_pretrained(
            base_model,
            self.finetuned_path,
            device_map="cpu"  # Force CPU cho adapter
        )
        print("‚úÖ LoRA adapters loaded")
        
        # KH√îNG merge ƒë·ªÉ ti·∫øt ki·ªám RAM
        print("\nüí° Keeping adapters separate (saves RAM)")
        
        print("\n" + "="*60)
        print("üéâ MODEL READY! (ƒê√É C√ì TR√ç NH·ªö)")
        print("="*60)
        print("üí° Model n√†y ƒë√£ h·ªçc t·ª´ d·ªØ li·ªáu c·ªßa b·∫°n")
        print("üí° L·∫ßn sau load s·∫Ω nhanh h∆°n (cache)")
        print("="*60 + "\n")
        
    def generate(self, prompt: str, max_new_tokens: int = 128) -> str:
        """Generate text t·ª´ model ƒë√£ fine-tune - OPTIMIZED"""
        if self._model is None:
            raise RuntimeError("Model ch∆∞a ƒë∆∞·ª£c load. G·ªçi load_model() tr∆∞·ªõc.")
        
        inputs = self._tokenizer(prompt, return_tensors="pt")
        
        # Move to same device as model
        device = next(self._model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # OPTIMIZED: Faster generation
        outputs = self._model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.1,           # Low temperature = more deterministic
            do_sample=True,
            top_p=0.9,
            num_beams=1,               # ‚úÖ No beam search = faster
            early_stopping=True,       # ‚úÖ Stop early if done
            pad_token_id=self._tokenizer.eos_token_id,
            use_cache=True             # ‚úÖ Use KV cache = faster
        )
        
        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response[len(prompt):].strip()

# Singleton instance
finetuned_model_loader = FinetunedModelLoader()
