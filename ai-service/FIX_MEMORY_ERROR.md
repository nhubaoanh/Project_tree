# ğŸ”§ FIX: Paging File Too Small Error

## Lá»—i
```
OSError: The paging file is too small for this operation to complete. (os error 1455)
```

## NguyÃªn nhÃ¢n
Model Qwen2.5-Coder-7B-Instruct (7 billion parameters) cáº§n ~14GB RAM Ä‘á»ƒ load, nhÆ°ng Windows paging file (virtual memory) quÃ¡ nhá».

---

## âœ… GIáº¢I PHÃP 1: TÄƒng Virtual Memory (KHUYÃŠN DÃ™NG)

### BÆ°á»›c 1: Má»Ÿ System Properties
1. Nháº¥n `Windows + R`
2. GÃµ `sysdm.cpl` vÃ  Enter
3. Chá»n tab **Advanced**
4. Click **Settings** trong pháº§n Performance

### BÆ°á»›c 2: TÄƒng Virtual Memory
1. Chá»n tab **Advanced**
2. Click **Change** trong pháº§n Virtual Memory
3. **Bá» tick** "Automatically manage paging file size for all drives"
4. Chá»n á»• Ä‘Ä©a (thÆ°á»ng lÃ  C:)
5. Chá»n **Custom size**:
   - **Initial size**: 16384 MB (16GB)
   - **Maximum size**: 32768 MB (32GB)
6. Click **Set**
7. Click **OK**
8. **Restart mÃ¡y tÃ­nh**

### BÆ°á»›c 3: Cháº¡y láº¡i AI Service
```bash
cd ai-service
python main.py
```

---

## âœ… GIáº¢I PHÃP 2: DÃ¹ng Model Nhá» HÆ¡n

Náº¿u mÃ¡y RAM tháº¥p (<16GB), dÃ¹ng model nhá» hÆ¡n:

### Option A: Qwen2.5-Coder-1.5B (Nháº¹ nháº¥t)
```python
# ai-service/.env
MODEL_NAME=Qwen/Qwen2.5-Coder-1.5B-Instruct
```

### Option B: Qwen2.5-Coder-3B
```python
# ai-service/.env
MODEL_NAME=Qwen/Qwen2.5-Coder-3B-Instruct
```

### So sÃ¡nh models:

| Model | Parameters | RAM cáº§n | Tá»‘c Ä‘á»™ | Accuracy |
|-------|-----------|---------|--------|----------|
| 1.5B  | 1.5B      | ~4GB    | Nhanh  | Tháº¥p     |
| 3B    | 3B        | ~8GB    | Trung bÃ¬nh | Trung bÃ¬nh |
| 7B    | 7B        | ~14GB   | Cháº­m   | Cao      |

---

## âœ… GIáº¢I PHÃP 3: Load Model vá»›i 8-bit Quantization

Giáº£m memory xuá»‘ng 50%:

```python
# ai-service/model_loader.py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

self._model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    cache_dir=MODEL_CACHE_DIR,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

**CÃ i Ä‘áº·t thÃªm:**
```bash
pip install bitsandbytes
```

---

## âœ… GIáº¢I PHÃP 4: Load Model tá»«ng pháº§n (Device Map)

```python
# ai-service/model_loader.py
self._model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    cache_dir=MODEL_CACHE_DIR,
    device_map="auto",  # Tá»± Ä‘á»™ng phÃ¢n bá»•
    low_cpu_mem_usage=True,  # Giáº£m RAM usage
    torch_dtype=torch.float16,
    trust_remote_code=True
)
```

---

## ğŸ¯ KHUYáº¾N NGHá»Š

### Náº¿u RAM < 16GB:
1. **TÄƒng Virtual Memory** (Giáº£i phÃ¡p 1)
2. **DÃ¹ng model 1.5B hoáº·c 3B** (Giáº£i phÃ¡p 2)

### Náº¿u RAM >= 16GB:
1. **TÄƒng Virtual Memory** (Giáº£i phÃ¡p 1)
2. Giá»¯ nguyÃªn model 7B

### Náº¿u cÃ³ GPU:
1. CÃ i CUDA: https://developer.nvidia.com/cuda-downloads
2. Set `DEVICE=cuda` trong `.env`
3. Model sáº½ load lÃªn GPU (nhanh hÆ¡n nhiá»u)

---

## ğŸ“Š Kiá»ƒm tra RAM hiá»‡n táº¡i

```bash
# Windows PowerShell
Get-WmiObject Win32_ComputerSystem | Select-Object TotalPhysicalMemory

# Hoáº·c
systeminfo | findstr /C:"Total Physical Memory"
```

---

## â“ Troubleshooting

### Lá»—i váº«n cÃ²n sau khi tÄƒng Virtual Memory?
- Äáº£m báº£o Ä‘Ã£ **restart mÃ¡y**
- Kiá»ƒm tra á»• Ä‘Ä©a cÃ²n Ä‘á»§ dung lÆ°á»£ng (cáº§n Ã­t nháº¥t 32GB free)
- ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c Ä‘ang cháº¡y

### Model load cháº­m?
- BÃ¬nh thÆ°á»ng, láº§n Ä‘áº§u load model 7B máº¥t 2-5 phÃºt
- CÃ¡c láº§n sau nhanh hÆ¡n (Ä‘Ã£ cache)

### Out of Memory khi generate?
- Giáº£m `max_new_tokens` trong config
- DÃ¹ng `torch.float16` thay vÃ¬ `float32`
- DÃ¹ng 8-bit quantization

