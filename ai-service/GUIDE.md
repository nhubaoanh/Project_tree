# ğŸ¤– AI SERVICE - HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG

## ğŸ¯ AI Service LÃ  GÃ¬?

Chuyá»ƒn cÃ¢u há»i tiáº¿ng Viá»‡t â†’ SQL query

**VÃ­ dá»¥:**
```
Input:  "CÃ³ bao nhiÃªu ngÆ°á»i trong gia pháº£?"
Output: "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ?"
```

---

## âš¡ CHáº Y NHANH (5 PHÃšT)

### 1. CÃ i Ä‘áº·t
```bash
cd ai-service
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh `.env`
```env
DB_HOST=localhost
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=treefamily
API_PORT=7000
```

### 3. Cháº¡y
```bash
python main.py
```

**Server cháº¡y táº¡i:** `http://localhost:7000`

---

## ğŸ“¡ API ENDPOINTS

### 1. Health Check
```bash
curl http://localhost:7000/health
```

**Response:**
```json
{
  "status": "ok",
  "model_loaded": true,
  "db_connected": true
}
```

### 2. Há»i CÃ¢u Há»i
```bash
curl -X POST http://localhost:7000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "CÃ³ bao nhiÃªu ngÆ°á»i trong gia pháº£?",
    "dongHoId": "xxx",
    "execute": true
  }'
```

**Response:**
```json
{
  "success": true,
  "question": "CÃ³ bao nhiÃªu ngÆ°á»i trong gia pháº£?",
  "sql": "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ?",
  "confidence": "95.0%",
  "results": [{"COUNT(*)": 150}],
  "total_rows": 1,
  "message": "TÃ¬m tháº¥y 1 káº¿t quáº£"
}
```

### 3. Xem CÃ¢u Há»i ÄÃ£ Thu Tháº­p
```bash
curl http://localhost:7000/logs/questions
```

### 4. Xem Káº¿t Quáº£ Query
```bash
curl http://localhost:7000/logs/results
```

---

## ğŸ“ Cáº¤U TRÃšC FILES

```
ai-service/
â”œâ”€â”€ main.py                      â† API Server (cháº¡y file nÃ y)
â”œâ”€â”€ model_loader_finetuned.py    â† Load model Ä‘Ã£ train
â”œâ”€â”€ sql_generator.py             â† Generate SQL
â”œâ”€â”€ query_executor.py            â† Execute SQL
â”œâ”€â”€ prompt_builder.py            â† Build prompt
â”œâ”€â”€ config.py                    â† Config
â”œâ”€â”€ .env                         â† Biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ requirements.txt             â† Dependencies
â”‚
â”œâ”€â”€ models/                      â† Base model (3GB - auto download)
â”œâ”€â”€ finetuned_model/             â† Model Ä‘Ã£ train (16.7MB)
â”œâ”€â”€ dataset/                     â† Training data
â””â”€â”€ logs/                        â† User questions
```

---

## ğŸ“ TRAINING MODEL

### Táº¡i Sao Pháº£i Train?

**TrÆ°á»›c train:**
- Model khÃ´ng biáº¿t tÃªn table
- SQL sai hoáº·c khÃ´ng cháº¡y
- Äá»™ chÃ­nh xÃ¡c: 60-70%

**Sau train:**
- Model nhá»› schema database
- SQL chÃ­nh xÃ¡c
- Äá»™ chÃ­nh xÃ¡c: 90-95% âœ…

### CÃ¡ch Train (Colab)

#### 1. Chuáº©n bá»‹ dataset
File `dataset/member.json`:
```json
[
  {
    "question": "CÃ³ bao nhiÃªu ngÆ°á»i?",
    "sql": "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ?"
  }
]
```

#### 2. Upload lÃªn Colab
```python
from google.colab import files
uploaded = files.upload()  # Chá»n member.json
```

#### 3. Cháº¡y training script
```python
# Install
!pip install transformers peft datasets torch

# Load model
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")

# Apply LoRA
from peft import LoraConfig, get_peft_model
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05
)
model = get_peft_model(model, lora_config)

# Train
from transformers import Trainer, TrainingArguments
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="./finetuned_model",
        num_train_epochs=10,  # TÄƒng náº¿u loss váº«n cao
        per_device_train_batch_size=1,
        learning_rate=2e-4
    ),
    train_dataset=dataset
)
trainer.train()

# Save
model.save_pretrained("./finetuned_model")
tokenizer.save_pretrained("./finetuned_model")
```

#### 4. Download
```python
!zip -r finetuned_model.zip finetuned_model/
from google.colab import files
files.download('finetuned_model.zip')
```

#### 5. Copy vÃ o project
```bash
unzip finetuned_model.zip
cp -r finetuned_model/ ai-service/
```

### Kiá»ƒm Tra Training

**Training log tá»‘t:**
```
Epoch 1: loss = 0.699
Epoch 5: loss = 0.187
Epoch 10: loss = 0.112  â† Tá»‘t! (< 0.15)
```

**Training log xáº¥u:**
```
Epoch 1: loss = 0.699
Epoch 3: loss = 0.650  â† KhÃ´ng giáº£m!
```

**Náº¿u loss khÃ´ng giáº£m:**
- TÄƒng epochs (10 â†’ 20)
- TÄƒng learning_rate (2e-4 â†’ 5e-4)
- Kiá»ƒm tra dataset cÃ³ Ä‘Ãºng khÃ´ng

---

## ğŸ§  CÃCH HOáº T Äá»˜NG

### 1. Load Model
```
Base Model (3GB) + LoRA Adapter (16.7MB) = Model Ä‘Ã£ train
```

**LoRA lÃ  gÃ¬?**
- Chá»‰ train 0.1% parameters (tiáº¿t kiá»‡m RAM)
- File nhá» (16.7MB thay vÃ¬ 3GB)
- Cháº¡y Ä‘Æ°á»£c trÃªn mÃ¡y yáº¿u

### 2. Generate SQL
```
User: "CÃ³ bao nhiÃªu ngÆ°á»i?"
  â†“
Prompt Builder: Táº¡o prompt vá»›i schema + examples
  â†“
Model: Generate SQL
  â†“
SQL: "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ?"
```

### 3. Execute SQL
```
SQL + Parameters â†’ MySQL â†’ Results
```

### 4. Log
```
LÆ°u cÃ¢u há»i vÃ o logs/questions.txt
LÆ°u káº¿t quáº£ vÃ o logs/query_results.jsonl
```

---

## ğŸ”§ TROUBLESHOOTING

### Lá»—i: Port 7000 Ä‘Ã£ dÃ¹ng
```bash
# Windows
netstat -ano | findstr :7000
taskkill /PID <pid> /F

# Hoáº·c Ä‘á»•i port trong .env
API_PORT=7001
```

### Lá»—i: Model khÃ´ng load
```bash
# Kiá»ƒm tra folder
ls finetuned_model/

# Pháº£i cÃ³ 10 files, trong Ä‘Ã³:
# - adapter_model.safetensors (> 10 MB)
# - adapter_config.json
# - tokenizer.json
```

### Lá»—i: RAM khÃ´ng Ä‘á»§
**Giáº£i phÃ¡p:**
1. ÄÃ³ng cÃ¡c app khÃ¡c
2. TÄƒng virtual memory (Windows)
3. Cháº¡y trÃªn Colab

### Lá»—i: SQL sai
**Giáº£i phÃ¡p:**
1. ThÃªm cÃ¢u há»i vÃ o `dataset/member.json`
2. Train láº¡i model
3. Test láº¡i

---

## ğŸ’¡ TIPS

### Cáº£i Thiá»‡n Äá»™ ChÃ­nh XÃ¡c

1. **Thu tháº­p cÃ¢u há»i thá»±c táº¿**
   ```bash
   curl http://localhost:7000/logs/questions
   ```

2. **ThÃªm vÃ o dataset**
   ```json
   {
     "question": "CÃ¢u há»i tá»« user",
     "sql": "SQL Ä‘Ãºng"
   }
   ```

3. **Train láº¡i**
   - Upload lÃªn Colab
   - Cháº¡y training script
   - Download model má»›i

### TÄƒng Epochs Khi NÃ o?

**NÃŠN tÄƒng khi:**
- Loss váº«n cao (> 0.3) sau 3 epochs
- Loss váº«n giáº£m Ä‘á»u
- KhÃ´ng cÃ³ overfitting

**KHÃ”NG nÃªn tÄƒng khi:**
- Loss Ä‘Ã£ tháº¥p (< 0.15)
- Loss khÃ´ng giáº£m ná»¯a
- Loss tÄƒng láº¡i (overfitting)

---

## ğŸ“Š PERFORMANCE

### Load Time
- Láº§n Ä‘áº§u: 2-3 phÃºt (download base model)
- Láº§n sau: 10-20 giÃ¢y (tá»« cache)

### Query Time
- **Láº§n 1:** 2-4 giÃ¢y (generate má»›i)
- **Láº§n 2+:** 0.01 giÃ¢y (tá»« cache) âš¡
- Simple query: 1-2 giÃ¢y
- Complex query: 2-4 giÃ¢y

### Accuracy
- Simple questions: 95-100%
- Complex questions: 85-95%
- Overall: 90-95%

### Tá»‘i Æ¯u ÄÃ£ Ãp Dá»¥ng
- âœ… Táº¯t `sqlparse.format()` (nhanh 0.5-1s)
- âœ… Giáº£m `max_new_tokens` 512â†’128 (nhanh 2x)
- âœ… Optimize generate params (nhanh 0.5-1s)
- âœ… Cache cÃ¢u há»i (nhanh 200x láº§n 2+)

**Xem chi tiáº¿t:** [PERFORMANCE.md](PERFORMANCE.md)

---

## ğŸ”„ MAINTENANCE

### HÃ ng NgÃ y
- Kiá»ƒm tra service health
- Monitor logs

### HÃ ng Tuáº§n
- Backup logs
- Review cÃ¢u há»i phá»• biáº¿n

### HÃ ng ThÃ¡ng
- Export dataset
- Train láº¡i náº¿u cÃ³ nhiá»u cÃ¢u há»i má»›i
- Backup model

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- **Transformers:** https://huggingface.co/docs/transformers
- **PEFT (LoRA):** https://huggingface.co/docs/peft
- **Qwen Model:** https://huggingface.co/Qwen

---

**Enjoy! ğŸš€**
