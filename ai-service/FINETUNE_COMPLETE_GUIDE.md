# ğŸ“ HÆ¯á»šNG DáºªN FINE-TUNE MODEL - CHI TIáº¾T Tá»ª A-Z

## ğŸ“š Má»¤C Lá»¤C

1. [Fine-tune lÃ  gÃ¬?](#fine-tune-lÃ -gÃ¬)
2. [Táº¡i sao cáº§n fine-tune?](#táº¡i-sao-cáº§n-fine-tune)
3. [CÃ¡ch hoáº¡t Ä‘á»™ng](#cÃ¡ch-hoáº¡t-Ä‘á»™ng)
4. [Chuáº©n bá»‹ dataset](#chuáº©n-bá»‹-dataset)
5. [Fine-tune step by step](#fine-tune-step-by-step)
6. [ÄÃ¡nh giÃ¡ model](#Ä‘Ã¡nh-giÃ¡-model)
7. [Deploy model má»›i](#deploy-model-má»›i)

---

## ğŸ¤” FINE-TUNE LÃ€ GÃŒ?

### Äá»‹nh nghÄ©a Ä‘Æ¡n giáº£n:

```
Pre-trained Model (Model gá»‘c)
    â†“
+ Your Data (Data cá»§a báº¡n)
    â†“
= Fine-tuned Model (Model Ä‘Ã£ há»c thÃªm)
```

### VÃ­ dá»¥ dá»… hiá»ƒu:

```
Giá»‘ng nhÆ° há»c sinh:

Pre-trained: Há»c sinh Ä‘Ã£ há»c toÃ¡n cÆ¡ báº£n (1+1=2, 2+2=4)
Fine-tune: Há»c thÃªm toÃ¡n nÃ¢ng cao (phÆ°Æ¡ng trÃ¬nh, Ä‘áº¡o hÃ m)
Result: Há»c sinh giá»i toÃ¡n hÆ¡n

Model cÅ©ng váº­y:

Pre-trained: Model Ä‘Ã£ biáº¿t SQL cÆ¡ báº£n
Fine-tune: Há»c thÃªm SQL cá»§a dÃ²ng há» báº¡n
Result: Model hiá»ƒu cÃ¢u há»i cá»§a báº¡n tá»‘t hÆ¡n
```

---

## ğŸ¯ Táº I SAO Cáº¦N FINE-TUNE?

### So sÃ¡nh Pre-trained vs Fine-tuned:

| TiÃªu chÃ­ | Pre-trained | Fine-tuned |
|----------|-------------|------------|
| **Accuracy** | 70-75% | 85-95% |
| **Hiá»ƒu thuáº­t ngá»¯ riÃªng** | âŒ KhÃ´ng | âœ… CÃ³ |
| **Xá»­ lÃ½ cÃ¢u phá»©c táº¡p** | âš ï¸ Trung bÃ¬nh | âœ… Tá»‘t |
| **Setup time** | 5 phÃºt | 2-3 giá» |
| **Cáº§n dataset** | KhÃ´ng | 100-500 examples |

### VÃ­ dá»¥ cá»¥ thá»ƒ:

```python
# Pre-trained model
Question: "CÃ³ bao nhiÃªu ngÆ°á»i trong gia pháº£?"
SQL: "SELECT COUNT(*) FROM thanhvien WHERE dongHoId = ?"
Accuracy: 75% âœ…

Question: "Ai lÃ  chÃ¡u ná»™i cá»§a Nguyá»…n VÄƒn A?"
SQL: "SELECT hoTen FROM thanhvien WHERE chaId = ..."  âŒ SAI!
# Model khÃ´ng hiá»ƒu "chÃ¡u ná»™i" = con cá»§a con trai

# Fine-tuned model (Ä‘Ã£ há»c thÃªm)
Question: "Ai lÃ  chÃ¡u ná»™i cá»§a Nguyá»…n VÄƒn A?"
SQL: "SELECT c.hoTen FROM thanhvien c 
      JOIN thanhvien p ON c.chaId = p.thanhVienId 
      WHERE p.chaId = (SELECT thanhVienId FROM thanhvien WHERE hoTen = 'Nguyá»…n VÄƒn A')
      AND p.gioiTinh = 1"  âœ… ÄÃšNG!
# Model Ä‘Ã£ há»c: chÃ¡u ná»™i = con cá»§a con trai (gioiTinh = 1)
```

---

## ğŸ” CÃCH HOáº T Äá»˜NG

### SÆ¡ Ä‘á»“ tá»•ng quan:

```
1. Pre-trained Model (Qwen 1.5B)
   ÄÃ£ há»c tá»« internet (code, text, SQL...)
   Weights: 3.5GB
   â†“
2. Your Dataset
   100-500 cÃ¢u há»i + SQL cá»§a dÃ²ng há» báº¡n
   â†“
3. Fine-tuning Process
   Model há»c thÃªm tá»« dataset cá»§a báº¡n
   Update weights (chá»‰ 1 pháº§n nhá»)
   â†“
4. Fine-tuned Model
   Weights: 3.5GB + 50-200MB (LoRA adapter)
   Hiá»ƒu cÃ¢u há»i cá»§a báº¡n tá»‘t hÆ¡n
```

### Chi tiáº¿t ká»¹ thuáº­t:

#### 1. Pre-trained Model

```python
# Model gá»‘c cÃ³ 1.5 tá»· parameters
model = {
    "layer_1": {
        "weights": [...],  # 100M parameters
        "bias": [...]
    },
    "layer_2": {...},
    ...
    "layer_32": {...}
}

# Má»—i parameter lÃ  1 sá»‘ (weight)
# VÃ­ dá»¥: weight[0][0] = 0.5234
```

**Táº¡i sao cÃ³ 1.5 tá»· parameters?**
```
Model cÃ ng lá»›n â†’ cÃ ng thÃ´ng minh
NhÆ°ng cÅ©ng â†’ cÃ ng cháº­m, cÃ ng tá»‘n RAM

1.5B: Vá»«a Ä‘á»§ thÃ´ng minh, vá»«a cháº¡y Ä‘Æ°á»£c trÃªn CPU
7B: ThÃ´ng minh hÆ¡n, nhÆ°ng cáº§n GPU
```

#### 2. Fine-tuning (Full vs LoRA)

**Full Fine-tuning (KhÃ´ng khuyáº¿n nghá»‹):**
```python
# Update Táº¤T Cáº¢ 1.5 tá»· parameters
for param in model.parameters():
    param.requires_grad = True  # Cho phÃ©p update

# Train
for epoch in range(3):
    for batch in dataset:
        loss = model(batch)
        loss.backward()  # TÃ­nh gradient
        optimizer.step()  # Update weights

# Káº¿t quáº£:
# - Tá»‘n RAM: 14GB (model) + 14GB (gradients) = 28GB
# - Tá»‘n thá»i gian: 2-3 giá»
# - File output: 3.5GB (toÃ n bá»™ model má»›i)
```

**LoRA Fine-tuning (Khuyáº¿n nghá»‹):**
```python
# Chá»‰ update 1 pháº§n nhá» (0.1% parameters)
# ThÃªm "adapter" layers

model_original = load_pretrained_model()  # 1.5B params
adapter = LoRAAdapter(rank=16)            # 1.5M params (0.1%)

# Train chá»‰ adapter
for param in model_original.parameters():
    param.requires_grad = False  # Freeze

for param in adapter.parameters():
    param.requires_grad = True   # Train

# Káº¿t quáº£:
# - Tá»‘n RAM: 4GB (model) + 500MB (adapter) = 4.5GB
# - Tá»‘n thá»i gian: 30-60 phÃºt
# - File output: 50-200MB (chá»‰ adapter)
```

**Táº¡i sao LoRA tá»‘t hÆ¡n?**
```
Full Fine-tuning:
âœ… Accuracy cao nháº¥t
âŒ Tá»‘n RAM (28GB)
âŒ Tá»‘n thá»i gian (2-3 giá»)
âŒ File lá»›n (3.5GB)

LoRA:
âœ… Accuracy gáº§n báº±ng (chá»‰ kÃ©m 1-2%)
âœ… Ãt RAM (4.5GB)
âœ… Nhanh (30-60 phÃºt)
âœ… File nhá» (50-200MB)
```

#### 3. CÃ¡ch LoRA hoáº¡t Ä‘á»™ng

```python
# Model gá»‘c
output = W * input  # W: weight matrix (1000x1000)

# LoRA thÃªm 2 ma tráº­n nhá»
output = W * input + (A * B) * input
# A: 1000x16 (rank=16)
# B: 16x1000
# A*B â‰ˆ W_update (xáº¥p xá»‰ update cá»§a W)

# Khi inference:
W_new = W + A * B  # Merge adapter vÃ o model
output = W_new * input
```

**Táº¡i sao rank=16?**
```
rank=4: Nhanh nháº¥t, accuracy tháº¥p nháº¥t
rank=8: CÃ¢n báº±ng
rank=16: Khuyáº¿n nghá»‹ (accuracy tá»‘t, váº«n nhanh)
rank=32: Accuracy cao nháº¥t, cháº­m hÆ¡n
```

---

## ğŸ“Š CHUáº¨N Bá»Š DATASET

### BÆ°á»›c 1: Thu tháº­p cÃ¢u há»i thá»±c táº¿

**Nguá»“n:**
- User thá»±c táº¿ há»i gÃ¬?
- CÃ¡c cÃ¢u há»i phá»• biáº¿n?
- CÃ¡c cÃ¢u há»i khÃ³?

**CÃ¡ch thu tháº­p:**

```python
# ThÃªm logging vÃ o main.py
@app.post("/ask")
async def ask(request: QueryRequest):
    # Log cÃ¢u há»i
    with open('logs/questions.txt', 'a', encoding='utf-8') as f:
        timestamp = datetime.now().isoformat()
        f.write(f"{timestamp}|{request.question}\n")
    
    # ... existing code
```

**Sau 1-2 tuáº§n:**
```
logs/questions.txt:
2026-01-25T10:30:00|CÃ³ bao nhiÃªu ngÆ°á»i trong gia pháº£?
2026-01-25T10:35:00|Nguyá»…n VÄƒn A sinh nÄƒm nÃ o?
2026-01-25T10:40:00|Ai lÃ  con cá»§a Tráº§n Thá»‹ B?
...
(100-500 cÃ¢u há»i)
```

### BÆ°á»›c 2: Táº¡o SQL cho má»—i cÃ¢u há»i

**File:** `ai-service/dataset/questions.json`

```json
[
  {
    "id": 1,
    "question": "CÃ³ bao nhiÃªu ngÆ°á»i trong gia pháº£?",
    "sql": "SELECT COUNT(*) as tong_so FROM thanhvien WHERE dongHoId = ? AND active_flag = 1",
    "category": "easy",
    "verified": true,
    "notes": "CÃ¢u há»i cÆ¡ báº£n, COUNT"
  },
  {
    "id": 2,
    "question": "Nguyá»…n VÄƒn A sinh nÄƒm nÃ o?",
    "sql": "SELECT YEAR(ngaySinh) as nam_sinh FROM thanhvien WHERE hoTen = 'Nguyá»…n VÄƒn A' AND dongHoId = ? AND active_flag = 1",
    "category": "easy",
    "verified": true,
    "notes": "Truy váº¥n thÃ´ng tin cÆ¡ báº£n"
  },
  {
    "id": 3,
    "question": "Ai lÃ  con cá»§a Nguyá»…n VÄƒn A?",
    "sql": "SELECT hoTen FROM thanhvien WHERE (chaId = (SELECT thanhVienId FROM thanhvien WHERE hoTen = 'Nguyá»…n VÄƒn A' AND dongHoId = ? AND active_flag = 1) OR meId = (SELECT thanhVienId FROM thanhvien WHERE hoTen = 'Nguyá»…n VÄƒn A' AND dongHoId = ? AND active_flag = 1)) AND dongHoId = ? AND active_flag = 1",
    "category": "medium",
    "verified": true,
    "notes": "Quan há»‡ cha-con, cáº§n subquery"
  },
  {
    "id": 4,
    "question": "Ai lÃ  chÃ¡u ná»™i cá»§a Nguyá»…n VÄƒn A?",
    "sql": "SELECT c.hoTen FROM thanhvien c JOIN thanhvien p ON c.chaId = p.thanhVienId WHERE p.chaId = (SELECT thanhVienId FROM thanhvien WHERE hoTen = 'Nguyá»…n VÄƒn A' AND dongHoId = ? AND active_flag = 1) AND p.gioiTinh = 1 AND c.dongHoId = ? AND c.active_flag = 1",
    "category": "hard",
    "verified": true,
    "notes": "Quan há»‡ 2 báº­c, cáº§n JOIN + Ä‘iá»u kiá»‡n giá»›i tÃ­nh"
  }
  // ... 100-500 examples
]
```

**Tips:**
- Báº¯t Ä‘áº§u vá»›i 50 examples
- Test accuracy
- ThÃªm dáº§n lÃªn 100, 200, 500
- Äa dáº¡ng hÃ³a: easy, medium, hard

### BÆ°á»›c 3: PhÃ¢n loáº¡i vÃ  validate

```python
# File: validate_dataset.py

import json

def validate_dataset():
    with open('dataset/questions.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    stats = {
        "total": len(data),
        "easy": 0,
        "medium": 0,
        "hard": 0,
        "verified": 0,
        "unverified": 0
    }
    
    for item in data:
        # Count by category
        stats[item['category']] += 1
        
        # Count verified
        if item.get('verified', False):
            stats['verified'] += 1
        else:
            stats['unverified'] += 1
        
        # Validate SQL
        if not item['sql'].strip():
            print(f"âŒ Empty SQL: {item['id']}")
        
        if '?' not in item['sql']:
            print(f"âš ï¸  No parameter: {item['id']}")
    
    print("=" * 60)
    print("ğŸ“Š DATASET STATISTICS")
    print("=" * 60)
    print(f"Total: {stats['total']}")
    print(f"Easy: {stats['easy']} ({stats['easy']/stats['total']*100:.1f}%)")
    print(f"Medium: {stats['medium']} ({stats['medium']/stats['total']*100:.1f}%)")
    print(f"Hard: {stats['hard']} ({stats['hard']/stats['total']*100:.1f}%)")
    print(f"Verified: {stats['verified']} ({stats['verified']/stats['total']*100:.1f}%)")
    print("=" * 60)

if __name__ == "__main__":
    validate_dataset()
```

**Cháº¡y:**
```bash
python validate_dataset.py
```

---

## ğŸš€ FINE-TUNE STEP BY STEP

### BÆ°á»›c 1: CÃ i Ä‘áº·t thÆ° viá»‡n

```bash
pip install transformers datasets peft accelerate bitsandbytes
```

**Giáº£i thÃ­ch:**
- `transformers`: Hugging Face library (load model, train)
- `datasets`: Xá»­ lÃ½ dataset
- `peft`: LoRA implementation
- `accelerate`: TÄƒng tá»‘c training
- `bitsandbytes`: Quantization (optional)

### BÆ°á»›c 2: Chuáº©n bá»‹ dataset

**File:** `ai-service/prepare_dataset.py`


```python
import json
from datasets import Dataset
from config import DATABASE_SCHEMA

def prepare_dataset():
    print("=" * 60)
    print("ğŸ“Š PREPARING DATASET")
    print("=" * 60)
    
    # 1. Load questions
    print("\n1. Loading questions...")
    with open('dataset/questions.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… Loaded {len(data)} examples")
    
    # 2. Format for training
    print("\n2. Formatting data...")
    formatted_data = []
    
    for item in data:
        # Táº¡o prompt giá»‘ng production
        prompt = f"""You are a SQL expert. Convert Vietnamese question to SQL query.

Database Schema:
{DATABASE_SCHEMA}

Question: {item['question']}
SQL:"""
        
        formatted_data.append({
            "input": prompt,
            "output": item['sql'],
            "category": item.get('category', 'unknown'),
            "id": item.get('id', 0)
        })
    
    print(f"âœ… Formatted {len(formatted_data)} examples")
    
    # 3. Create dataset
    print("\n3. Creating dataset...")
    dataset = Dataset.from_list(formatted_data)
    print(f"âœ… Dataset created: {len(dataset)} examples")
    
    # 4. Split train/validation/test (80/10/10)
    print("\n4. Splitting dataset...")
    train_test = dataset.train_test_split(test_size=0.2, seed=42)
    test_val = train_test['test'].train_test_split(test_size=0.5, seed=42)
    
    final_dataset = {
        'train': train_test['train'],
        'validation': test_val['train'],
        'test': test_val['test']
    }
    
    print(f"âœ… Train: {len(final_dataset['train'])} examples")
    print(f"âœ… Validation: {len(final_dataset['validation'])} examples")
    print(f"âœ… Test: {len(final_dataset['test'])} examples")
    
    # 5. Save
    print("\n5. Saving dataset...")
    for split, data in final_dataset.items():
        data.save_to_disk(f'dataset/processed/{split}')
        print(f"âœ… Saved {split}")
    
    print("\n" + "=" * 60)
    print("âœ… DATASET PREPARED!")
    print("=" * 60)
    print("\nNext step: python finetune.py")

if __name__ == "__main__":
    prepare_dataset()
```

**Cháº¡y:**
```bash
python prepare_dataset.py
```

**Output:**
```
ğŸ“Š PREPARING DATASET
1. Loading questions...
âœ… Loaded 100 examples
2. Formatting data...
âœ… Formatted 100 examples
3. Creating dataset...
âœ… Dataset created: 100 examples
4. Splitting dataset...
âœ… Train: 80 examples
âœ… Validation: 10 examples
âœ… Test: 10 examples
5. Saving dataset...
âœ… Saved train
âœ… Saved validation
âœ… Saved test
âœ… DATASET PREPARED!
```

### BÆ°á»›c 3: Fine-tune vá»›i LoRA

**File:** `ai-service/finetune.py`

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_from_disk
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import os

# ============================================
# CONFIGURATION
# ============================================
BASE_MODEL = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
OUTPUT_DIR = "./models/finetuned-sql-generator"
DATASET_DIR = "./dataset/processed"

# LoRA config
LORA_R = 16              # Rank (4, 8, 16, 32)
LORA_ALPHA = 32          # Alpha (thÆ°á»ng = 2 * rank)
LORA_DROPOUT = 0.05      # Dropout
TARGET_MODULES = ["q_proj", "v_proj"]  # Layers to train

# Training config
NUM_EPOCHS = 3           # Sá»‘ epochs
BATCH_SIZE = 4           # Batch size (giáº£m náº¿u OOM)
LEARNING_RATE = 2e-4     # Learning rate
WARMUP_STEPS = 100       # Warmup steps

def finetune():
    print("=" * 60)
    print("ğŸ“ FINE-TUNING MODEL")
    print("=" * 60)
    
    # ============================================
    # 1. LOAD BASE MODEL
    # ============================================
    print("\nğŸ“¥ Step 1: Loading base model...")
    print(f"Model: {BASE_MODEL}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"âœ… Model loaded")
    print(f"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
    
    # ============================================
    # 2. CONFIGURE LORA
    # ============================================
    print("\nâš™ï¸  Step 2: Configuring LoRA...")
    print(f"   Rank: {LORA_R}")
    print(f"   Alpha: {LORA_ALPHA}")
    print(f"   Target modules: {TARGET_MODULES}")
    
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # Prepare model for training
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    
    # Print trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"âœ… LoRA configured")
    print(f"   Trainable params: {trainable_params / 1e6:.2f}M ({trainable_params / total_params * 100:.2f}%)")
    print(f"   Total params: {total_params / 1e9:.2f}B")
    
    # ============================================
    # 3. LOAD DATASET
    # ============================================
    print("\nğŸ“Š Step 3: Loading dataset...")
    
    train_dataset = load_from_disk(f'{DATASET_DIR}/train')
    val_dataset = load_from_disk(f'{DATASET_DIR}/validation')
    
    print(f"âœ… Dataset loaded")
    print(f"   Train: {len(train_dataset)} examples")
    print(f"   Validation: {len(val_dataset)} examples")
    
    # Tokenize function
    def tokenize_function(examples):
        # Combine input and output
        texts = [f"{inp}\n{out}" for inp, out in zip(examples['input'], examples['output'])]
        
        # Tokenize
        result = tokenizer(
            texts,
            truncation=True,
            max_length=512,
            padding=False
        )
        
        return result
    
    # Tokenize datasets
    print("\nğŸ”„ Tokenizing datasets...")
    train_dataset = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names
    )
    val_dataset = val_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=val_dataset.column_names
    )
    print("âœ… Tokenization complete")
    
    # ============================================
    # 4. TRAINING ARGUMENTS
    # ============================================
    print("\nğŸ‹ï¸  Step 4: Setting up training...")
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=4,  # Effective batch = 4*4=16
        learning_rate=LEARNING_RATE,
        warmup_steps=WARMUP_STEPS,
        logging_steps=10,
        save_steps=100,
        eval_steps=100,
        evaluation_strategy="steps",
        save_total_limit=3,
        fp16=True,
        report_to="none",
        load_best_model_at_end=True
    )
    
    print(f"âœ… Training configured")
    print(f"   Epochs: {NUM_EPOCHS}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Learning rate: {LEARNING_RATE}")
    
    # ============================================
    # 5. DATA COLLATOR
    # ============================================
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LM, not masked LM
    )
    
    # ============================================
    # 6. TRAINER
    # ============================================
    print("\nğŸš€ Step 5: Creating trainer...")
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator
    )
    
    print("âœ… Trainer created")
    
    # ============================================
    # 7. TRAIN!
    # ============================================
    print("\n" + "=" * 60)
    print("ğŸš€ STARTING TRAINING")
    print("=" * 60)
    print("\nâ° This will take 30-60 minutes on CPU, 5-10 minutes on GPU")
    print("ğŸ“Š Watch the loss decrease over time")
    print("\n")
    
    trainer.train()
    
    # ============================================
    # 8. SAVE MODEL
    # ============================================
    print("\nğŸ’¾ Step 6: Saving fine-tuned model...")
    
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"âœ… Model saved to: {OUTPUT_DIR}")
    
    # ============================================
    # 9. SUMMARY
    # ============================================
    print("\n" + "=" * 60)
    print("âœ… FINE-TUNING COMPLETED!")
    print("=" * 60)
    print(f"\nğŸ“ Model location: {OUTPUT_DIR}")
    print(f"ğŸ“Š Files:")
    print(f"   - adapter_config.json (LoRA config)")
    print(f"   - adapter_model.safetensors (Trained weights)")
    print(f"   - tokenizer files")
    
    print(f"\nğŸ¯ Next steps:")
    print(f"   1. Evaluate model: python evaluate.py")
    print(f"   2. Update config.py: MODEL_NAME = '{OUTPUT_DIR}'")
    print(f"   3. Restart service: python main.py")
    print(f"   4. Test accuracy improvement")
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    finetune()
```

**Cháº¡y:**
```bash
python finetune.py
```

**Output (vÃ­ dá»¥):**
```
ğŸ“ FINE-TUNING MODEL
ğŸ“¥ Step 1: Loading base model...
âœ… Model loaded
   Parameters: 1.54B

âš™ï¸  Step 2: Configuring LoRA...
âœ… LoRA configured
   Trainable params: 1.57M (0.10%)
   Total params: 1.54B

ğŸ“Š Step 3: Loading dataset...
âœ… Dataset loaded
   Train: 80 examples
   Validation: 10 examples

ğŸš€ STARTING TRAINING
Epoch 1/3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  Loss: 2.345 â†’ 1.234
Epoch 2/3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  Loss: 1.234 â†’ 0.876
Epoch 3/3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  Loss: 0.876 â†’ 0.654

âœ… FINE-TUNING COMPLETED!
ğŸ“ Model location: ./models/finetuned-sql-generator
```

**Giáº£i thÃ­ch output:**
- **Loss giáº£m**: 2.345 â†’ 0.654 (model Ä‘ang há»c!)
- **Loss cÃ ng tháº¥p**: Model cÃ ng chÃ­nh xÃ¡c
- **Náº¿u loss khÃ´ng giáº£m**: Dataset cÃ³ váº¥n Ä‘á» hoáº·c learning rate sai

---

## ğŸ“Š ÄÃNH GIÃ MODEL

### File: `ai-service/evaluate.py`

```python
from datasets import load_from_disk
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import json

MODEL_PATH = "./models/finetuned-sql-generator"
BASE_MODEL = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

def evaluate():
    print("=" * 60)
    print("ğŸ“Š EVALUATING MODEL")
    print("=" * 60)
    
    # 1. Load model
    print("\n1. Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model = PeftModel.from_pretrained(base_model, MODEL_PATH)
    print("âœ… Model loaded")
    
    # 2. Load test dataset
    print("\n2. Loading test dataset...")
    test_dataset = load_from_disk('dataset/processed/test')
    print(f"âœ… Test dataset: {len(test_dataset)} examples")
    
    # 3. Evaluate
    print("\n3. Evaluating...")
    results = {
        "total": len(test_dataset),
        "correct": 0,
        "errors": []
    }
    
    for i, example in enumerate(test_dataset):
        # Extract question and expected SQL
        input_text = example['input']
        question = input_text.split('Question: ')[1].split('\nSQL:')[0]
        expected_sql = example['output']
        
        # Generate SQL
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                temperature=0.0,
                do_sample=False
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_sql = generated_text.split('SQL:')[-1].strip()
        
        # Compare
        if generated_sql.strip() == expected_sql.strip():
            results['correct'] += 1
        else:
            results['errors'].append({
                "question": question,
                "expected": expected_sql,
                "generated": generated_sql
            })
        
        if (i + 1) % 5 == 0:
            print(f"   Processed {i + 1}/{results['total']}")
    
    # 4. Calculate metrics
    accuracy = results['correct'] / results['total'] * 100
    
    print("\n" + "=" * 60)
    print("ğŸ“Š EVALUATION RESULTS")
    print("=" * 60)
    print(f"Total: {results['total']}")
    print(f"Correct: {results['correct']}")
    print(f"Accuracy: {accuracy:.2f}%")
    print("=" * 60)
    
    # 5. Save errors
    if results['errors']:
        with open('evaluation_errors.json', 'w', encoding='utf-8') as f:
            json.dump(results['errors'], f, ensure_ascii=False, indent=2)
        print(f"\nâŒ {len(results['errors'])} errors saved to: evaluation_errors.json")
    else:
        print("\nâœ… No errors! Perfect accuracy!")
    
    return accuracy

if __name__ == "__main__":
    evaluate()
```

**Cháº¡y:**
```bash
python evaluate.py
```

---

## ğŸš€ DEPLOY MODEL Má»šI

### BÆ°á»›c 1: Cáº­p nháº­t config.py

```python
# File: config.py

# Thay Ä‘á»•i tá»«:
MODEL_NAME = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

# ThÃ nh:
MODEL_NAME = "./models/finetuned-sql-generator"
```

### BÆ°á»›c 2: Cáº­p nháº­t model_loader.py

```python
# File: model_loader.py

from peft import PeftModel

def load_model(self):
    # Check if fine-tuned model
    is_finetuned = os.path.exists(os.path.join(MODEL_NAME, "adapter_config.json"))
    
    if is_finetuned:
        logger.info("ğŸ“ Loading FINE-TUNED model")
        
        # Load base model
        base_model_name = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Load LoRA adapter
        self._model = PeftModel.from_pretrained(base_model, MODEL_NAME)
        self._tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        logger.info("âœ… Fine-tuned model loaded!")
    else:
        # Load pre-trained model (existing code)
        ...
```

### BÆ°á»›c 3: Restart service

```bash
python main.py
```

### BÆ°á»›c 4: Test

```bash
curl -X POST http://localhost:7000/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"Ai lÃ  chÃ¡u ná»™i cá»§a Nguyá»…n VÄƒn A?","dongHoId":"DH001","execute":true}'
```

---

## ğŸ“Š SO SÃNH Káº¾T QUáº¢

| Metric | Pre-trained | Fine-tuned | Cáº£i thiá»‡n |
|--------|-------------|------------|-----------|
| Accuracy | 70-75% | 85-95% | +15-20% |
| Easy questions | 90% | 95% | +5% |
| Medium questions | 70% | 90% | +20% |
| Hard questions | 50% | 80% | +30% |
| Hiá»ƒu thuáº­t ngá»¯ riÃªng | âŒ | âœ… | âœ… |

---

## ğŸ’¡ TIPS

### 1. Báº¯t Ä‘áº§u nhá»
- 50 examples â†’ Test
- 100 examples â†’ Test
- 500 examples â†’ Deploy

### 2. Quality > Quantity
- 100 examples cháº¥t lÆ°á»£ng cao > 500 examples kÃ©m

### 3. Äa dáº¡ng hÃ³a
- Nhiá»u loáº¡i cÃ¢u há»i
- Nhiá»u cÃ¡ch há»i khÃ¡c nhau
- Nhiá»u Ä‘á»™ khÃ³

### 4. Monitor training
- Loss pháº£i giáº£m
- Náº¿u loss tÄƒng â†’ learning rate quÃ¡ cao
- Náº¿u loss khÃ´ng Ä‘á»•i â†’ dataset cÃ³ váº¥n Ä‘á»

---

## ğŸ¯ TÃ“M Táº®T

Fine-tune lÃ  quÃ¡ trÃ¬nh train thÃªm model trÃªn data cá»§a báº¡n Ä‘á»ƒ tÄƒng accuracy. Vá»›i LoRA, báº¡n chá»‰ cáº§n:
- 100-500 examples
- 30-60 phÃºt training
- 4-8GB RAM
- Accuracy tÄƒng 15-20%

ÄÃ¢y lÃ  cÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ model hiá»ƒu cÃ¢u há»i cá»§a dÃ²ng há» báº¡n!
